{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5884e5e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdc2640",
   "metadata": {},
   "source": [
    "### Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fc7013",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGS_DIR = \"feature_interp_logs\"\n",
    "EXPERIMENTS_DIR = \"feature_interp_exp\"\n",
    "NEURONPEDIA_EXPORTS_DIR = \"neuronpedia_exports\"\n",
    "TRANSLUCE_EXPORTS_DIR = \"transluce_exports\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d96e46",
   "metadata": {},
   "source": [
    "### Keys and Tokens (Fill In)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd110c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAI_KEY = \"\" # GEMINI\n",
    "HF_TOKEN = \"\" # HF\n",
    "# OPENAI (should be in env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd581bbb",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62549bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import gc\n",
    "import pickle\n",
    "import requests\n",
    "import time\n",
    "import traceback\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, List, DefaultDict, Tuple\n",
    "from json.decoder import JSONDecodeError\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import functools\n",
    "from loguru import logger\n",
    "\n",
    "# Hugging Face and Models\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from huggingface_hub import hf_hub_download, notebook_login, login\n",
    "from openai import OpenAI, RateLimitError\n",
    "import google.generativeai as gai\n",
    "import datasets\n",
    "\n",
    "\n",
    "# SAE and Transformer Lens\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "from transformer_lens.utils import test_prompt, tokenize_and_concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5b6733",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# trigger garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# free all memory in use by cuda\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Get the current GPU memory usage\n",
    "allocated_memory = torch.cuda.memory_allocated()\n",
    "reserved_memory = torch.cuda.memory_reserved()\n",
    "\n",
    "logger.debug(f\"Allocated memory: {allocated_memory / (1024 ** 3):.2f} GB\")\n",
    "logger.debug(f\"Reserved memory: {reserved_memory / (1024 ** 3):.2f} GB\")\n",
    "\n",
    "torch.set_grad_enabled(False) # avoid blowing up mem\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "logger.debug(f\"Device: {device}\")\n",
    "\n",
    "\n",
    "def free(verbose=False):\n",
    "    # trigger garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "    # free all memory in use by cuda\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Get the current GPU memory usage\n",
    "    if verbose:\n",
    "        logger.info(\"Allocated\", torch.cuda.memory_allocated())\n",
    "        logger.info(\"Reserved\", torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcf0e8a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(LOGS_DIR):\n",
    "    os.mkdir(LOGS_DIR)\n",
    "    \n",
    "def trace_logger(log_filename=\"trace.log\"):\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            log_full_path = os.path.join(LOGS_DIR, log_filename)\n",
    "            log_sink = None\n",
    "            try:\n",
    "                log_sink = logger.add(log_full_path, level=\"TRACE\")\n",
    "                result = func(*args, **kwargs)\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Exception in {func.__name__}: {e}\")\n",
    "                raise\n",
    "            finally:\n",
    "                if log_sink is not None:\n",
    "                    logger.remove(log_sink)\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d04bb74",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def pd_ext():\n",
    "    pd.set_option('display.max_rows', None)  # Show all rows\n",
    "    pd.set_option('display.max_columns', None)  # Show all columns\n",
    "    pd.set_option('display.max_colwidth', None)  # No truncation of column content\n",
    "    pd.set_option('display.expand_frame_repr', False)  # Avoid wrapping data in the notebook\n",
    "\n",
    "def pd_reset():\n",
    "    pd.reset_option(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ff7bcc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def format_time(secs):\n",
    "    if isinstance(secs, str):\n",
    "        return secs\n",
    "    minutes = int(secs // 60)\n",
    "    seconds = int(secs % 60)\n",
    "    return f'{minutes:02}:{seconds:02}'\n",
    "\n",
    "def get_its_per_second_test(its_per_second):\n",
    "    if its_per_second == 0:\n",
    "        return \"?it/s\"\n",
    "    if its_per_second < 1:\n",
    "        return f\"{1/its_per_second:.2f}s/it\"\n",
    "    return f\"{its_per_second:.2f}it/s\"\n",
    "    \n",
    "\n",
    "def _tqdm(iterable, total=None, width=20, fill='█', print_end='\\r'):\n",
    "    start = time.time()\n",
    "\n",
    "    if total is None:\n",
    "        total = len(iterable)\n",
    "\n",
    "    five_count = 0\n",
    "    last_five = []\n",
    "\n",
    "    def print_bar(i, n, prev_time):\n",
    "        now = time.time()\n",
    "        time_since_start = now - start\n",
    "        time_since_last = now - prev_time\n",
    "        mean_diff = 0\n",
    "\n",
    "        if prev_time == 0 or i == total:\n",
    "            projected_time = 0\n",
    "        else:\n",
    "            projected_time_all = (time_since_start / i) * total - time_since_start\n",
    "\n",
    "            if five_count >= 1:\n",
    "                diffs = []\n",
    "                last_five.append(now)\n",
    "                for j in range(len(last_five)-1):\n",
    "                    diffs.append(last_five[j+1] - last_five[j])\n",
    "\n",
    "                last_five.pop(-1)\n",
    "\n",
    "                mean_diff = sum(diffs) / len(diffs)\n",
    "                projected_time_fives = mean_diff * (total - i)\n",
    "            else:\n",
    "                projected_time_fives = time_since_last * (total - i)\n",
    "\n",
    "            projected_time = projected_time_all*0.7 + projected_time_fives*0.3\n",
    "\n",
    "        print(f'\\r{(i/total)*100:.0f}%|{\"█\" * n + \" \" * (width - n)}| {i}/{total} [{format_time(time_since_start)}<{format_time(projected_time)},  {get_its_per_second_test(i / time_since_start)}]', end=print_end, flush=True)\n",
    "\n",
    "    print(\" \"*100, end=print_end, flush=True)\n",
    "    print_bar(0, 0, 0)\n",
    "    for i, item in enumerate(iterable):\n",
    "        prev = time.time()\n",
    "\n",
    "        last_five.append(prev)\n",
    "        five_count += 1\n",
    "\n",
    "        if five_count > 5:\n",
    "            five_count -= 1\n",
    "            last_five.pop(0)\n",
    "        \n",
    "        yield item\n",
    "        print_bar(i+1, int((i + 1) / total * width), prev)\n",
    "\n",
    "    print()\n",
    "\n",
    "\n",
    "class Pbar:\n",
    "    def __init__(self, total, width=20, fill='█', print_end='\\r'):\n",
    "        self.start = time.time()\n",
    "        self.total = total\n",
    "        self.width = width\n",
    "        self.fill = fill\n",
    "        self.print_end = print_end\n",
    "        self.five_count = 0\n",
    "        self.last_five = []\n",
    "        self.count = 0\n",
    "        self.desc = \"\"\n",
    "\n",
    "        print(\" \"*100, end=self.print_end, flush=True)\n",
    "        self.print_bar(0, 0, 0)\n",
    "\n",
    "        self.prev = time.time()\n",
    "        self.last_five.append(self.prev)\n",
    "\n",
    "    def print_bar(self, i, n, prev_time):\n",
    "        now = time.time()\n",
    "        time_since_start = now - self.start\n",
    "        time_since_last = now - prev_time\n",
    "        mean_diff = 0\n",
    "\n",
    "        if prev_time == 0 or i == self.total:\n",
    "            projected_time = 0\n",
    "        elif i > self.total:\n",
    "            projected_time = \"?\"\n",
    "        else:\n",
    "            projected_time_all = (time_since_start / i) * self.total - time_since_start\n",
    "\n",
    "            if self.five_count >= 1:\n",
    "                diffs = []\n",
    "                self.last_five.append(now)\n",
    "                for j in range(len(self.last_five)-1):\n",
    "                    diffs.append(self.last_five[j+1] - self.last_five[j])\n",
    "\n",
    "                self.last_five.pop(-1)\n",
    "\n",
    "                mean_diff = sum(diffs) / len(diffs)\n",
    "                projected_time_fives = mean_diff * (self.total - i)\n",
    "            else:\n",
    "                projected_time_fives = time_since_last * (self.total - i)\n",
    "\n",
    "            projected_time = projected_time_all*0.7 + projected_time_fives*0.3\n",
    "\n",
    "        if self.desc:\n",
    "            prefix = f\"{self.desc}:  \"\n",
    "        else:\n",
    "            prefix = \"\"\n",
    "\n",
    "        print(f'\\r{prefix}{(i/self.total)*100:.0f}%|{\"█\" * n + \" \" * (self.width - n)}| {i}/{self.total} [{format_time(time_since_start)}<{format_time(projected_time)},  {get_its_per_second_test(i / time_since_start)}]', end=self.print_end, flush=True)\n",
    "\n",
    "    def update(self, n):\n",
    "        self.count += n\n",
    "        \n",
    "        self.print_bar(self.count, min(int((self.count) / self.total * self.width), self.width), self.prev)\n",
    "\n",
    "        self.prev = time.time()\n",
    "        self.last_five.append(self.prev)\n",
    "        self.five_count += 1\n",
    "\n",
    "        if self.five_count > 5:\n",
    "            self.five_count -= 1\n",
    "            self.last_five.pop(0)\n",
    "\n",
    "    def set_description(self, desc):\n",
    "        self.desc = desc\n",
    "\n",
    "def tqdm_(iterable=None, total=None):\n",
    "    if iterable is None:\n",
    "        return Pbar(total)\n",
    "    return _tqdm(iterable, total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ce226e",
   "metadata": {},
   "source": [
    "### SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8f7025",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_all_saes_df():\n",
    "  # Each row is a \"release\" which has multiple SAEs which may have different configs / match different hook points in a model.\n",
    "  all_saes_df = pd.DataFrame.from_records({k:v.__dict__ for k,v in get_pretrained_saes_directory().items()}).T\n",
    "  all_saes_df.drop(columns=[\"expected_var_explained\", \"expected_l0\",\n",
    "                            \"config_overrides\", \"conversion_func\"], inplace=True)\n",
    "  return all_saes_df\n",
    "\n",
    "def get_feature_api(modelId, saeId, feature):\n",
    "    \"\"\"\n",
    "    NEURONPEDIA GET /api/feature/export\n",
    "    https://www.neuronpedia.org/api-doc#tag/features/GET/api/feature/{modelId}/{layer}/{index}\n",
    "    e.g. https://www.neuronpedia.org/api/feature/gpt2-small/0-res-jb/14057\n",
    "    \"\"\"\n",
    "    url = f\"https://www.neuronpedia.org/api/feature/{modelId}/{saeId}/{feature}\"\n",
    "    response = requests.get(url)\n",
    "    result = response.json()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6865785",
   "metadata": {},
   "source": [
    "### Feature & Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c013d2f5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def size_to_int(size: str):\n",
    "    \"\"\"\n",
    "    Converts an SAE size in string format to its integer value (e.g. \"16k\" -> 16384)\n",
    "    \"\"\"\n",
    "    assert size[-1] == 'k', \"Invalid size string format\"\n",
    "    num = int(size[:len(size) - 1])\n",
    "    raw_value = num * (2 ** 10)\n",
    "    true_power = round(math.log2(raw_value))\n",
    "    return 2 ** true_power\n",
    "\n",
    "def write_pkl_file(data, file_name):\n",
    "    pickle.dump(data, open(file_name, \"wb\"))  \n",
    "\n",
    "def read_pkl_file(file_name):\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        data = pickle.load(f)  \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61e2a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Activation:\n",
    "    id: str\n",
    "    token_values: List[Tuple[str, float]]\n",
    "    max_value: float\n",
    "    min_value: float\n",
    "    max_value_token_index: int = None\n",
    "    loss_values: List[float] = None\n",
    "    \n",
    "    def get_tokens(self) -> List[str]:\n",
    "        tokens = [tv[0] for tv in self.token_values]\n",
    "        return tokens\n",
    "\n",
    "    def get_values(self) -> List[float]:\n",
    "        values = [tv[1] for tv in self.token_values]\n",
    "        return values\n",
    "    \n",
    "    def get_tokens_str(self) -> str:\n",
    "        return \"\".join(self.get_tokens())\n",
    "    \n",
    "    def get_max_token_values(self, w: int = 5):\n",
    "        start = max(self.max_value_token_index - w, 0)\n",
    "        end = min(self.max_value_token_index + w, len(self.token_values))\n",
    "        return self.token_values[start:end]\n",
    "    \n",
    "    def get_max_tokens_str(self, w: int = 5) -> str:\n",
    "        # w = 5 is the \"stacked\", a larger value will be \"snippet\" (in neuronpedia's jargon)\n",
    "        start = self.max_value_token_index - w\n",
    "        end = self.max_value_token_index + w\n",
    "        return \"\".join(self.get_tokens()[start:end])\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "    \n",
    "    def __str__(self):\n",
    "        if self.max_value_token_index is not None:\n",
    "            return f\"Max={self.token_values[self.max_value_token_index]}, Sentence={self.get_max_token_values()}\"\n",
    "        return f\"Tokens={self.get_tokens()}\"\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(str(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cb3fa2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Feature:\n",
    "    model_id: str      # e.g. gemma-2-2b\n",
    "    feature: int       # e.g. 1846\n",
    "    layer: str         # e.g. 11\n",
    "    type: str          # e.g. att, mlp, res\n",
    "    activations: List[Activation] = None\n",
    "    \n",
    "    sae_id: str = None       # from saes_df, e.g. layer_11/width_16k/average_l0_80\n",
    "    sae_release: str = None  # from saes_df, e.g. gemma-scope-2b-pt-mlp\t  \n",
    "    size: str = \"\"         # e.g. 16k, 65k\n",
    "    \n",
    "    def get_pedia_dashboard_url(self, np_sae_id) -> str:\n",
    "        return f\"https://www.neuronpedia.org/{self.model_id}/{np_sae_id}/{self.feature}\"\n",
    "    \n",
    "    def get_size_int(self):\n",
    "        if self.size != \"\":\n",
    "            return size_to_int(self.size)\n",
    "        return 0\n",
    "    \n",
    "    def get_max_activating_examples(self, k: int = 5) -> List[Activation]:\n",
    "        unique_activations = list(set(self.activations))\n",
    "        sorted_activations = sorted(unique_activations, key=lambda x: x.max_value, reverse=True)\n",
    "        return sorted_activations[:k]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Feature {self.type}-{self.size}/{self.layer}/{self.feature}\"\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(str(self))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80a45ec",
   "metadata": {},
   "source": [
    "#### SAE activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c39f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sae_export_dir(modelId: str, saeId: str) -> str:\n",
    "    saeIdNoLayer = saeId.split('-', 1)[1]\n",
    "    model_dir = f\"{modelId}-{saeIdNoLayer}\"\n",
    "    return os.path.join(NEURONPEDIA_EXPORTS_DIR, model_dir, saeId)\n",
    "\n",
    "def get_feature_export_json(model_export_dir: str, feature: int):\n",
    "    files = os.listdir(model_export_dir)\n",
    "    for file in files:\n",
    "        # Extract the range from the filename\n",
    "        try:\n",
    "            start, end = map(int, file.rstrip('.json').split('-'))\n",
    "            # Check if the feature is within the range\n",
    "            if start <= feature < end:\n",
    "                return file\n",
    "        except ValueError:\n",
    "            continue  # Skip files that don't match the expected format\n",
    "    return None\n",
    "\n",
    "def get_feature_json_index(json_file, feature: int) -> int:\n",
    "    start, end = map(int, json_file.rstrip('.json').split('-'))\n",
    "    return feature - start\n",
    "\n",
    "def get_feature_json_data(json_path: str, feature: int):\n",
    "    \"\"\"\n",
    "    Retrieved the data for feature \n",
    "    \"\"\"\n",
    "    json_file = os.path.basename(json_path)\n",
    "    json_feature_index = get_feature_json_index(json_file, feature)\n",
    "    \n",
    "    # Load the JSON file\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    feature_data = data[json_feature_index]\n",
    "    logger.trace(f\"Got data for feature {feature} which is at index {json_feature_index} in the json {json_file}\")\n",
    "    return feature_data\n",
    "    \n",
    "def get_export_data(modelId: str, saeId: str, feature: int) -> Dict:\n",
    "    \"\"\"\n",
    "    Retrieved the data for the given feature from neuronpedia's exports\n",
    "    \"\"\"\n",
    "    logger.trace(f\"Getting export for {modelId}/{saeId}/{feature}\")\n",
    "    sae_export_dir = get_sae_export_dir(modelId, saeId)\n",
    "    if not os.path.exists(sae_export_dir):\n",
    "        logger.trace(f\"No appropriate export dir found for {feature} in {sae_export_dir}\")\n",
    "        return {}\n",
    "    json_file = get_feature_export_json(sae_export_dir, feature)\n",
    "    if json_file is None:\n",
    "        logger.trace(f\"No appropriate json file found for {feature} in {sae_export_dir}\")\n",
    "        return {}\n",
    "    \n",
    "    json_path = os.path.join(sae_export_dir, json_file)\n",
    "    data = get_feature_json_data(json_path, feature)\n",
    "    return data\n",
    "\n",
    "def json_to_activations(activations_data) -> List[Activation]:\n",
    "    activations = []\n",
    "    for a_dict in activations_data:\n",
    "        tokens = a_dict['tokens']\n",
    "        values = a_dict['values']\n",
    "        token_values = list(zip(tokens, values))\n",
    "        a = Activation('', token_values, a_dict['maxValue'], a_dict['minValue'], \n",
    "                       a_dict['maxValueTokenIndex'], a_dict['lossValues'])\n",
    "        activations.append(a)\n",
    "    logger.trace(f\"Retrieved {len(activations)} activations\")\n",
    "    return activations\n",
    "    \n",
    "def get_activations_data(modelId: str, saeId: str, feature: int) -> List[Activation]:\n",
    "    data = get_export_data(modelId, saeId, feature)\n",
    "    if not data:\n",
    "        return []\n",
    "    activations = json_to_activations(data['activations'])\n",
    "    return activations\n",
    "\n",
    "def get_act_neuronpedia(modelId: str, saeId: str, feature: int) -> List[Activation]:\n",
    "    \"\"\"\n",
    "    Returns the activations for the feature\n",
    "    \"\"\"\n",
    "    # first try the explanations API (calling it or using the already cached data)\n",
    "    result = get_activations_data(modelId, saeId, feature)\n",
    "    if result == []:\n",
    "        api_result = get_feature_api(modelId, saeId, feature)\n",
    "        if api_result is None:\n",
    "            return []\n",
    "\n",
    "        result = json_to_activations(api_result['activations'])\n",
    "        logger.trace(f\"Got activations data from feature api\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22a7fef",
   "metadata": {},
   "source": [
    "#### Transluce activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840d1e75",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_act_transluce():\n",
    "    pkl_path = os.path.join(TRANSLUCE_EXPORTS_DIR, \"transluce_formatted_activations.pkl\")\n",
    "    with open(pkl_path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "    \n",
    "\n",
    "def get_act_transluce(layer: int, feature: int) -> List[Activation]:\n",
    "    data = load_act_transluce()\n",
    "    try:\n",
    "        result = data[(layer, feature)]\n",
    "        return result\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad04662d",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ed4b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Model:\n",
    "    \"\"\"matches the modelId in neuronepedia API calls, used to load model using from_pretrained(), e.g. gemma-2-2b\n",
    "    this is a valid model name in https://transformerlensorg.github.io/TransformerLens/generated/model_properties_table.html\"\"\"\n",
    "    model_id: str           \n",
    "    \n",
    "    \"\"\"a more friendly name for the model, e.g. for the same model_id (gpt2-small), \n",
    "    there are different available saes (v5, jb) and this helps us to distinguish them, a more unique name for the model\"\"\"\n",
    "    model_name: str     \n",
    "    \n",
    "    with_sae: bool\n",
    "    \n",
    "    \"\"\"used to search for the relevant releases in the general df of available SAEs\n",
    "    e.g. gemma-scope-2b-pt-{}, gpt2-small-{}-{}-v5-{}\"\"\"    \n",
    "    sae_release_prefix: str = None\n",
    "    \n",
    "    sae_sizes: List[str] = None   # e.g. [16k, 65k]    \n",
    "    sae_types: List[str] = None   # e.g. [mlp, res, att]\n",
    "    saes_df: pd.DataFrame = None\n",
    "    m: HookedSAETransformer = None\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"after loading the model, we update model_id - so we can use it properly in other places.\n",
    "        for neuronpedia the model id is 'llama3.1-8b' and not 'meta-llama/Llama-3.1-8B' \"\"\"\n",
    "        if self.model_id == \"llama3.1-8b\":\n",
    "            model_id = \"meta-llama/Llama-3.1-8B\"\n",
    "        elif self.model_id == \"llama3.1-8b-instruct\":\n",
    "            model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "        else:\n",
    "            model_id = self.model_id\n",
    "\n",
    "        if self.m is None:\n",
    "            self.m = HookedSAETransformer.from_pretrained(model_id, device=device)\n",
    "            \n",
    "    def is_gemma(self):\n",
    "        return self.model_id.startswith(\"gemma\")\n",
    "    \n",
    "    def is_llama(self):\n",
    "        return self.model_name == \"llama3.1-8b\"\n",
    "    \n",
    "    def is_gpt_jb(self):\n",
    "        return self.model_name == \"gpt2-small-jb\"\n",
    "    \n",
    "    def is_gpt_v5(self):\n",
    "        return self.model_name == \"gpt2-small-v5\"\n",
    "    \n",
    "    def get_size_int(self, size: str):\n",
    "        \"\"\"special case: d_model (768) * 32 = d_sae (24576)\n",
    "        this is done just to get a random feature in the appropriate range, Feature object stays with empty string type \"\"\"\n",
    "        return 24576 if self.is_gpt_jb() else size_to_int(size)\n",
    "            \n",
    "    def get_pile_dataset(self):\n",
    "        dataset = datasets.load_dataset(\"NeelNanda/pile-10k\", split=\"train\")\n",
    "        pile = tokenize_and_concatenate(dataset, self.m.tokenizer, streaming=False, max_length=32, column_name=\"text\", add_bos_token=True, num_proc=4)\n",
    "        pile = pile[:-1000][\"tokens\"]\n",
    "        return pile\n",
    "        \n",
    "    def get_features_pkl_name(self):\n",
    "        return f\"features-sample-{self.model_name}.pkl\" \n",
    "    \n",
    "    def write_features(self, data, out_file: str = None):\n",
    "        file_name = out_file if out_file is not None else self.get_features_pkl_name()\n",
    "        logger.info(f\"Writing {len(data)} features to {file_name}\")\n",
    "        write_pkl_file(data, file_name)\n",
    "    \n",
    "    def get_features(self, in_file: str = None):\n",
    "        file_name = in_file if in_file is not None else self.get_features_pkl_name()\n",
    "        logger.info(f\"Getting features from {file_name}\")\n",
    "        return read_pkl_file(file_name)\n",
    "    \n",
    "    def get_specific_layers(self) ->  List:\n",
    "        specific_layers = []\n",
    "        for type in self.sae_types:\n",
    "            for size in self.sae_sizes:\n",
    "                layers = [*[(type, layer, size) for layer in range(self.m.cfg.n_layers)]]\n",
    "                specific_layers.extend(layers)\n",
    "        return specific_layers\n",
    "    \n",
    "    # --------------- SAE ---------------\n",
    "    def extract_type(self, saes_df_row):\n",
    "        for t in self.sae_types:\n",
    "            if t in saes_df_row.release or t in saes_df_row.np_id:\n",
    "                return t\n",
    "        raise ValueError(f\"Found no valid type ({self.sae_types}) in given row {saes_df_row.release}\")\n",
    "\n",
    "    def extract_size(self, saes_df_row):\n",
    "        for s in self.sae_sizes:\n",
    "            if s in saes_df_row.np_id:\n",
    "                return s\n",
    "        raise ValueError(f\"Found no valid size ({self.sae_sizes}) in given row {saes_df_row.id}\")\n",
    "    \n",
    "    def get_np_sae_id(self, layer: str, type: str, size: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract the saeId for neuronpedia (from the previously loaded saes df)\n",
    "        \"\"\"\n",
    "        if self.saes_df is None:\n",
    "            raise ValueError(\"saes df does not exist, please call get_saes_info_specific_layers()\")    \n",
    "        # e.g. for gemma: 5-gemmascope-att-16k, for gpt: 1-res_mid_128k-oai\n",
    "        np_id = self.saes_df[(self.saes_df['layer'] == layer) &\n",
    "                            (self.saes_df['width'] == size) &\n",
    "                            ((self.saes_df['release'].str.contains(type)) | (self.saes_df['np_id'].str.contains(type)))][\"np_id\"]\n",
    "        if np_id.empty:\n",
    "            raise ValueError(f\"requested sae_id {type}/{layer}/{size} does not exist in current saes_df! check types and sizes defined: {self.sae_types}, {self.sae_sizes}\")    \n",
    "        sae_id = np_id.iloc[0]\n",
    "        sae_id = sae_id.removeprefix(f\"{self.model_id}/\")  # this is the format which can be inferred from neuronpedia's conventions\n",
    "        return sae_id\n",
    "    \n",
    "    def enrich_sae_ids(self, sae_ids_df):\n",
    "        if self.is_gemma():\n",
    "            sae_ids_df['layer'] = sae_ids_df['id'].apply(lambda x: x.split('/')[0].split('_')[1])\n",
    "            sae_ids_df['width'] = sae_ids_df['id'].apply(lambda x: x.split('/')[1].split('_')[1])\n",
    "            sae_ids_df['average_l0'] = sae_ids_df['id'].apply(lambda x: x.split('/')[2])\n",
    "        elif self.is_llama():\n",
    "            sae_ids_df['layer'] = sae_ids_df['np_id'].apply(lambda x: x.split('/')[1].split('-')[0])\n",
    "            sae_ids_df['width'] = sae_ids_df['np_id'].apply(lambda x: x.split('/')[1].split('-')[-1])\n",
    "        elif self.is_gpt_v5():\n",
    "            sae_ids_df['layer'] = sae_ids_df['id'].apply(lambda x: x.split('.')[1])\n",
    "            sae_ids_df['width'] = sae_ids_df['release'].apply(lambda x: x.split('-')[-1])\n",
    "        elif self.is_gpt_jb():\n",
    "            sae_ids_df['layer'] = sae_ids_df['id'].apply(lambda x: x.split('.')[1])\n",
    "            sae_ids_df['width'] = \"\"\n",
    "        \n",
    "        return sae_ids_df\n",
    "    \n",
    "    def get_sae_ids(self, all_saes_df):\n",
    "        \"\"\"\n",
    "        Given a specific release, get its sae ids\n",
    "        \"\"\"\n",
    "        # TODO: check using the \"model\" field - take all the SAEs where it matches self.model_id\n",
    "        if self.is_gemma():\n",
    "            # \"canonical\" is the sae used in neuronpedia (average_l0 close to 100 - https://huggingface.co/google/gemma-scope-2b-pt-mlp/blob/main/README.md)\n",
    "            saes_map = all_saes_df[(all_saes_df['release'].str.contains(self.sae_release_prefix)) &\n",
    "                                       (all_saes_df['release'].str.contains('canonical'))][[\"saes_map\", \"neuronpedia_id\"]]\n",
    "        elif self.is_gpt_v5():\n",
    "            saes_map = all_saes_df[(all_saes_df['release'].str.contains(self.sae_release_prefix)) &\n",
    "                                (all_saes_df['release'].str.contains(\"v5\"))][[\"saes_map\", \"neuronpedia_id\"]]\n",
    "        elif self.is_gpt_jb() or self.is_llama():\n",
    "            saes_map = all_saes_df[(all_saes_df['release'].str.contains(self.sae_release_prefix))][[\"saes_map\", \"neuronpedia_id\"]]\n",
    "            \n",
    "        df = pd.DataFrame(saes_map)\n",
    "        sae_ids_df = pd.DataFrame(columns=['id', 'release'])\n",
    "        for release in df.index:\n",
    "            ids = df.loc[release]['saes_map'].keys()\n",
    "            np_ids = df.loc[release]['neuronpedia_id'].values()  # the neuronpedia sae_id for API usage\n",
    "            temp_df = pd.DataFrame({'id': ids, 'np_id': np_ids, 'release': release})\n",
    "            sae_ids_df = pd.concat([sae_ids_df, temp_df])\n",
    "            \n",
    "        # problem splitting \"layer_n\" later because the in this case the string is simply \"embedding\", so we omit these rows\n",
    "        sae_ids_df = sae_ids_df[~(sae_ids_df['id'].str.contains('embedding'))]            \n",
    "        sae_ids_df = self.enrich_sae_ids(sae_ids_df)  \n",
    "        return sae_ids_df\n",
    "\n",
    "    def get_saes_info_specific_layers(self, specific_layers):\n",
    "        \"\"\"\n",
    "        Creating the main saes df for this model\n",
    "        \"\"\"\n",
    "        all_saes_df = get_all_saes_df()\n",
    "        sae_ids_df = self.get_sae_ids(all_saes_df)\n",
    "        sae_ids = []\n",
    "        for layer_type, layer_num, layer_width in specific_layers:\n",
    "            sae_id_temp = sae_ids_df[(sae_ids_df['layer'] == str(layer_num)) & \n",
    "                                     (sae_ids_df['width'] == str(layer_width)) &\n",
    "                                     # release - for gemma, gpt. np_id - for llama (because release doesn't contain full type name)\n",
    "                                     ((sae_ids_df['release'].str.contains(layer_type)) | (sae_ids_df['np_id'].str.contains(layer_type)))]\n",
    "            sae_id = sae_id_temp.iloc[0]['np_id']\n",
    "            sae_ids.append(sae_id)\n",
    "        # using np_id because apparently id can be sometimes not unique! (e.g. layer_11/width_16k/average_l0_80 exists both for ATT and RES!)\n",
    "        sae_single_ids_df = sae_ids_df[sae_ids_df['np_id'].isin(sae_ids)]\n",
    "        self.saes_df = sae_single_ids_df.reset_index(drop=True)\n",
    "        \n",
    "        \n",
    "    def get_feature_sample_sae(self, amount):\n",
    "        \"\"\"\n",
    "        :param amount: The amount of features from each SAE of the model to sample.\n",
    "        \"\"\"\n",
    "        all_features_sample = []\n",
    "        logger.info(f\"Getting {amount} features per layer ({len(self.saes_df)}) for {self.model_name}\")\n",
    "        for i in tqdm_(range(len(self.saes_df))):\n",
    "            x = self.saes_df.iloc[i]\n",
    "            size = self.extract_size(x)\n",
    "            type = self.extract_type(x)\n",
    "            j = 0            \n",
    "            attempts = 0\n",
    "            while j < amount:\n",
    "                attempts += 1\n",
    "                if attempts > 100:\n",
    "                    break\n",
    "                \n",
    "                size_int = self.get_size_int(size)\n",
    "                rand_feature = torch.randint(size_int, (1,))[0].item()                \n",
    "                np_sae_id = self.get_np_sae_id(x.layer, type, size)  \n",
    "                j += 1\n",
    "                \n",
    "                new_f = Feature(self.model_id, rand_feature, x.layer, type, sae_id=x.id, sae_release=x.release, size=size)\n",
    "                activations = get_act_neuronpedia(self.model_id, np_sae_id, new_f.feature)      \n",
    "                if activations:\n",
    "                    new_f = Feature(new_f.model_id, new_f.feature, new_f.layer, new_f.type, activations, new_f.sae_id, new_f.sae_release, new_f.size)\n",
    "\n",
    "                logger.trace(f\"Adding new feature {repr(new_f)}\")\n",
    "                all_features_sample.append(new_f)\n",
    "                \n",
    "            logger.trace(f\"Added {j} features, from {type}.{x.layer}.{size} using {x.release} SAE\")\n",
    "    \n",
    "    \n",
    "    def get_feature_sample_no_sae(self, amount, type=\"mlp\"):\n",
    "        \"\"\"\n",
    "        :param amount: The amount of features from each layer of the model to sample.\n",
    "        \"\"\"\n",
    "        all_features_sample = []\n",
    "        logger.info(f\"Getting {amount} features per layer ({self.m.cfg.n_layers}) for {self.model_name}\")\n",
    "        for layer in tqdm_(range(self.m.cfg.n_layers)):\n",
    "            j = 0            \n",
    "            sample = random.sample(range(self.m.cfg.d_mlp), amount)\n",
    "            for rand_feature in sample:\n",
    "                new_f = Feature(self.model_id, rand_feature, layer, type)\n",
    "                activations = get_act_transluce(layer, rand_feature)\n",
    "                if activations:\n",
    "                    new_f = Feature(new_f.model_id, new_f.feature, new_f.layer, new_f.type, activations)\n",
    "\n",
    "                logger.trace(f\"Adding new feature {repr(new_f)}\")\n",
    "                all_features_sample.append(new_f)\n",
    "                \n",
    "            logger.trace(f\"Added {j} features, from {type}.{layer}\")\n",
    "        \n",
    "        return all_features_sample\n",
    "    \n",
    "    def get_feature_sample_transluce(self):\n",
    "        all_features_sample = []\n",
    "        # TODO: consider creating a new activations .pkl with 80 features per layer and not 40\n",
    "        data = load_act_transluce()\n",
    "        logger.info(f\"Getting {len(data)} features for {self.model_name}\")\n",
    "        \n",
    "        for (layer, feature) in data:\n",
    "            new_f = Feature(self.model_id, feature, layer, \"mlp\", data[(layer, feature)])\n",
    "            logger.trace(f\"Adding new feature {repr(new_f)}\")\n",
    "            all_features_sample.append(new_f)\n",
    "            \n",
    "        return all_features_sample\n",
    "\n",
    "    def write_feature_sample(self, write=False, out_file=None):\n",
    "        \"\"\"\n",
    "        Exports a sample of features to a pickle file.\n",
    "        :param write: Whether to actually write the data to the file (think carefully before writing).\n",
    "        :param out_file: A custom output file path instead of the default one.\n",
    "        \"\"\"\n",
    "        if self.with_sae:\n",
    "            all_features_sample = self.get_feature_sample_sae(amount=40)\n",
    "        else:\n",
    "            all_features_sample = self.get_feature_sample_transluce()\n",
    "\n",
    "        random.shuffle(all_features_sample)\n",
    "        if write:\n",
    "            self.write_features(all_features_sample, out_file=out_file)\n",
    "\n",
    "        return all_features_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b30e8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    \"GEMMA-2-2B\":       Model(\"gemma-2-2b\", \"gemma-2-2b\", True, \"gemma-scope-2b-pt-\", [\"16k\", \"65k\"], [\"res\", \"mlp\"]), \n",
    "    \"GEMMA-2-9B\":       Model(\"gemma-2-9b\", \"gemma-2-9b\", True, \"gemma-scope-9b-pt-\", [\"16k\", \"131k\"], [\"res\", \"mlp\"]),  \n",
    "    \"LLAMA-3.1-8B\":     Model(\"llama3.1-8b\", \"llama3.1-8b\", True, \"llama_scope_lx\", [\"32k\"], [\"res\", \"mlp\"]),\n",
    "    \"GPT-2-SM-V5\":      Model(\"gpt2-small\", \"gpt2-small-v5\", True, \"gpt2-small-\", [\"32k\", \"128k\"], [\"resid-mid\", \"resid-post\", \"mlp-out\"]),\n",
    "    \"GPT-2-SM-JB\":      Model(\"gpt2-small\", \"gpt2-small-jb\", True, \"gpt2-small-res-jb\", [\"\"], [\"res\"]),\n",
    "    \"LLAMA-3.1-8B-IN\":  Model(\"llama3.1-8b-instruct\", \"llama3.1-8b-instruct\", False),    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85de78c3",
   "metadata": {},
   "source": [
    "### Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cb6fc8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class Explainer:\n",
    "    def __init__(self, remote: bool, default_remote_model=\"gpt-4o\", weak_remote_model=\"gpt-4o-mini\", default_local_model=\"meta-llama/Meta-Llama-3-70B-Instruct\", gemini=False):\n",
    "        self.remote = remote\n",
    "        self.weak_remote_model = weak_remote_model\n",
    "        self.gemini = gemini\n",
    "        if remote:\n",
    "            if gemini:\n",
    "                gai.configure(api_key=GAI_KEY)\n",
    "                self.remote_model = gai.GenerativeModel(\"models/gemini-1.5-pro-latest\")\n",
    "\n",
    "            else:\n",
    "                self.client = OpenAI()\n",
    "                self.remote_model = default_remote_model\n",
    "        else:\n",
    "            self.local_model = pipeline(\"text-generation\", model=default_local_model, device_map=\"auto\", max_length=10000)\n",
    "            self.local_model.tokenizer.pad_token_id = self.local_model.tokenizer.eos_token_id # why?\n",
    "\n",
    "\n",
    "    def __call__(self, prompts: List[Dict], weak=False):\n",
    "        if self.remote:\n",
    "            if self.gemini:\n",
    "                prompt = \"\"\n",
    "                for p in prompts:\n",
    "                    prompt += p[\"content\"] + \"\\n\"\n",
    "\n",
    "                return self.remote_model.generate_content(prompt).text\n",
    "\n",
    "            else:\n",
    "                assert weak, \"Only weak mode is supported for OpenAI API\"\n",
    "                remote_model = self.remote_model if not weak else self.weak_remote_model\n",
    "                completion = self.client.chat.completions.create(model=remote_model, messages=prompts)\n",
    "                return completion.choices[0].message.content\n",
    "        \n",
    "        else:\n",
    "            return self.local_model(prompts)[0][\"generated_text\"][1][\"content\"]\n",
    "\n",
    "\n",
    "explainer = Explainer(remote=True)\n",
    "gemini_explainer = Explainer(remote=True, gemini=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4365c2ed",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_description(sys_prompt, user_prompt, weak=True) -> str:\n",
    "    explanation = explainer([\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ], weak=weak)\n",
    "    \n",
    "    try:\n",
    "        json_content = explanation.strip(\"json\").strip('`').removeprefix('json\\n').removesuffix('\\n')\n",
    "        j = json.loads(json_content)\n",
    "        explanation = j[\"Explanation\"]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d357643",
   "metadata": {},
   "source": [
    "## Description Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff38ecce",
   "metadata": {},
   "source": [
    "### MaxAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787e2896",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Example:\n",
    "    activation_records: List[Activation]\n",
    "    explanation: str\n",
    "    \n",
    "example1 = Example(\n",
    "    activation_records=[\n",
    "        Activation(\n",
    "            id='',\n",
    "            token_values=[\n",
    "                (token, activation)\n",
    "                for token, activation in zip(\n",
    "                    [\n",
    "                        \"t\", \"urt\", \"ur\", \"ro\", \" is\", \" fab\", \"ulously\", \" funny\",\n",
    "                        \" and\", \" over\", \" the\", \" top\", \" as\", \" a\", \" '\", \"very\",\n",
    "                        \" sneaky\", \"'\", \" but\", \"ler\", \" who\", \" excel\", \"s\", \" in\",\n",
    "                        \" the\", \" art\", \" of\", \" impossible\", \" disappearing\", \"/\",\n",
    "                        \"re\", \"app\", \"earing\", \" acts\"\n",
    "                    ],\n",
    "                    [\n",
    "                        -0.71, -1.85, -2.39, -2.58, -1.34, -1.92, -1.69, -0.84,\n",
    "                        -1.25, -1.75, -1.42, -1.47, -1.51, -0.8, -1.89, -1.56,\n",
    "                        -1.63, 0.44, -1.87, -2.55, -2.09, -1.76, -1.33, -0.88,\n",
    "                        -1.63, -2.39, -2.63, -0.99, 2.83, -1.11, -1.19, -1.33,\n",
    "                        4.24, -1.51\n",
    "                    ],\n",
    "                )\n",
    "            ],\n",
    "            max_value=4.24,\n",
    "            min_value=-2.63,\n",
    "            max_value_token_index=32\n",
    "        ),\n",
    "        Activation(\n",
    "            id='',\n",
    "            token_values=[\n",
    "                (token, activation)\n",
    "                for token, activation in zip(\n",
    "                    [\n",
    "                        \"esc\", \"aping\", \" the\", \" studio\", \" ,\", \" pic\", \"col\",\n",
    "                        \"i\", \" is\", \" warm\", \"ly\", \" affecting\", \" and\", \" so\",\n",
    "                        \" is\", \" this\", \" ad\", \"roit\", \"ly\", \" minimalist\", \" movie\",\n",
    "                        \" .\"\n",
    "                    ],\n",
    "                    [\n",
    "                        -0.69, 4.12, 1.83, -2.28, -0.28, -0.79, -2.2, -2.03,\n",
    "                        -1.77, -1.71, -2.44, 1.6, -1, -0.38, -1.93, -2.09,\n",
    "                        -1.63, -1.94, -1.82, -1.64, -1.32, -1.92\n",
    "                    ],\n",
    "                )\n",
    "            ],\n",
    "            max_value=4.12,\n",
    "            min_value=-2.44,\n",
    "            max_value_token_index=1\n",
    "        ),\n",
    "    ],\n",
    "    explanation=\"present tense verbs ending in 'ing'\")\n",
    "    \n",
    "example2 = Example(\n",
    "    activation_records=[\n",
    "        Activation(\n",
    "            id='',\n",
    "            token_values=[\n",
    "                (\"as\", -0.14),\n",
    "                (\" sac\", -1.37),\n",
    "                (\"char\", -0.68),\n",
    "                (\"ine\", -2.27),\n",
    "                (\" movies\", -1.46),\n",
    "                (\" go\", -1.11),\n",
    "                (\" ,\", -0.9),\n",
    "                (\" this\", -2.48),\n",
    "                (\" is\", -2.07),\n",
    "                (\" likely\", -3.49),\n",
    "                (\" to\", -2.16),\n",
    "                (\" cause\", -1.79),\n",
    "                (\" massive\", -0.23),\n",
    "                (\" cardiac\", -0.04),\n",
    "                (\" arrest\", 4.46),\n",
    "                (\" if\", -1.02),\n",
    "                (\" taken\", -2.26),\n",
    "                (\" in\", -2.95),\n",
    "                (\" large\", -1.49),\n",
    "                (\" doses\", -1.46),\n",
    "                (\" .\", -0.6),\n",
    "            ],\n",
    "            max_value=4.46,\n",
    "            min_value=-3.49,\n",
    "            max_value_token_index=14,\n",
    "        ),\n",
    "        Activation(\n",
    "            id='',\n",
    "            token_values=[\n",
    "                (\"shot\", -0.09),\n",
    "                (\" perhaps\", -3.53),\n",
    "                (\"'\", -0.72),\n",
    "                (\"art\", -2.36),\n",
    "                (\"istically\", -1.05),\n",
    "                (\"'\", -1.12),\n",
    "                (\" with\", -2.49),\n",
    "                (\"handheld\", -2.14),\n",
    "                (\" cameras\", -1.98),\n",
    "                (\" and\", -1.59),\n",
    "                (\" apparently\", -2.62),\n",
    "                (\" no\", -2),\n",
    "                (\" movie\", -2.73),\n",
    "                (\" lights\", -2.87),\n",
    "                (\" by\", -3.23),\n",
    "                (\" jo\", -1.11),\n",
    "                (\"aquin\", -2.23),\n",
    "                (\" b\", -0.97),\n",
    "                (\"aca\", -2.28),\n",
    "                (\"-\", -2.37),\n",
    "                (\"as\", -1.5),\n",
    "                (\"ay\", -2.81),\n",
    "                (\" ,\", -1.73),\n",
    "                (\" the\", -3.14),\n",
    "                (\" low\", -2.61),\n",
    "                (\"-\", -1.7),\n",
    "                (\"budget\", -3.08),\n",
    "                (\" production\", -4),\n",
    "                (\" swings\", -0.71),\n",
    "                (\" annoy\", -2.48),\n",
    "                (\"ingly\", -1.39),\n",
    "                (\" between\", -1.96),\n",
    "                (\" vert\", -1.09),\n",
    "                (\"igo\", 4.37),\n",
    "                (\" and\", -0.74),\n",
    "                (\" opacity\", -0.5),\n",
    "                (\" .\", -0.62),\n",
    "            ],\n",
    "            max_value=4.37,\n",
    "            min_value=-4,\n",
    "            max_value_token_index=33,\n",
    "        ),\n",
    "    ],\n",
    "    explanation=\"words related to physical medical conditions\",\n",
    ")\n",
    "\n",
    "example3 = Example(\n",
    "    activation_records=[\n",
    "        Activation(\n",
    "            id='',\n",
    "            token_values=[\n",
    "                (\"the\", 0),\n",
    "                (\" sense\", 0),\n",
    "                (\" of\", 0),\n",
    "                (\" together\", 1),\n",
    "                (\"ness\", 2),\n",
    "                (\" in\", 0),\n",
    "                (\" our\", 0.23),\n",
    "                (\" town\", 0.5),\n",
    "                (\" is\", 0),\n",
    "                (\" strong\", 0),\n",
    "                (\" .\", 0),\n",
    "            ],\n",
    "            max_value=2,\n",
    "            min_value=0,\n",
    "            max_value_token_index=4,\n",
    "        ),\n",
    "        Activation(\n",
    "            id='',\n",
    "            token_values=[\n",
    "                (\"a\", -0.15),\n",
    "                (\" buoy\", -2.33),\n",
    "                (\"ant\", -1.4),\n",
    "                (\" romantic\", -2.17),\n",
    "                (\" comedy\", -2.53),\n",
    "                (\" about\", -0.85),\n",
    "                (\" friendship\", 0.23),\n",
    "                (\",\", -1.89),\n",
    "                (\" love\", 0.09),\n",
    "                (\",\", -0.47),\n",
    "                (\" and\", -0.5),\n",
    "                (\" the\", -0.58),\n",
    "                (\" truth\", -0.87),\n",
    "                (\" that\", 0.22),\n",
    "                (\" we\", 0.58),\n",
    "                (\"'re\", 1.34),\n",
    "                (\" all\", 0.98),\n",
    "                (\" in\", 2.21),\n",
    "                (\" this\", 2.84),\n",
    "                (\" together\", 1.7),\n",
    "                (\" .\", -0.89),\n",
    "            ],\n",
    "            max_value=2.84,\n",
    "            min_value=-2.53,\n",
    "            max_value_token_index=18,\n",
    "        ),\n",
    "    ],\n",
    "    explanation=\"phrases related to community\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52b93b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ACT_DESCRIPTION_PREFIX = \"the main thing this neuron does is find\"\n",
    "\n",
    "def relu(x: float) -> float:\n",
    "    return max(0.0, x)\n",
    "\n",
    "def normalize_activations(activation_record: List[float], max_activation: float) -> List[int]:\n",
    "    \"\"\"Convert raw neuron activations to integers on the range [0, 10].\"\"\"\n",
    "    if max_activation <= 0:\n",
    "        return [0 for x in activation_record]\n",
    "    # Relu is used to assume any values less than 0 are indicating the neuron is in the resting\n",
    "    # state. This is a simplifying assumption that works with relu/gelu.\n",
    "    return [min(10, math.floor(10 * relu(x) / max_activation)) for x in activation_record]\n",
    "\n",
    "def format_activation_record(activation_record: Activation, omit_zeros: bool) -> str:\n",
    "    tokens = activation_record.get_tokens()\n",
    "    normalized_activations = normalize_activations(activation_record.get_values(), activation_record.max_value)\n",
    "    if omit_zeros:\n",
    "        tokens = [\n",
    "            token for token, activation in zip(tokens, normalized_activations) if activation > 0\n",
    "        ]\n",
    "        normalized_activations = [x for x in normalized_activations if x > 0]\n",
    "    entries = []\n",
    "    assert len(tokens) == len(normalized_activations)\n",
    "    for token, activation in zip(tokens, normalized_activations):\n",
    "        activation_string = str(int(activation))\n",
    "        entries.append(f\"{token}\\t{activation_string}\")\n",
    "    return \"\\n\".join(entries)\n",
    "\n",
    "def format_activation_records(activation_records: List[Activation], omit_zeros: bool) -> str:\n",
    "    \"\"\"Format a list of activation records into a string.\"\"\"\n",
    "    return (\n",
    "        \"\\n<start>\\n\"\n",
    "        + \"\\n<end>\\n<start>\\n\".join(\n",
    "            [\n",
    "                format_activation_record(activation_record, omit_zeros=omit_zeros)\n",
    "                for activation_record in activation_records\n",
    "            ]\n",
    "        )\n",
    "        + \"\\n<end>\\n\"\n",
    "    )\n",
    "    \n",
    "def non_zero_activation_proportion(activation_records: List[Activation]) -> float:\n",
    "    \"\"\"Return the proportion of activation values that aren't zero.\"\"\"\n",
    "    total_activations_count = sum(\n",
    "        [len(activation_record.get_values()) for activation_record in activation_records]\n",
    "    )\n",
    "    normalized_activations = [\n",
    "        normalize_activations(activation_record.get_values(), activation_record.max_value)\n",
    "        for activation_record in activation_records\n",
    "    ]\n",
    "    non_zero_activations_count = sum(\n",
    "        [len([x for x in activations if x != 0]) for activations in normalized_activations]\n",
    "    )\n",
    "    return non_zero_activations_count / total_activations_count\n",
    "\n",
    "def add_per_neuron_explanation_prompt(\n",
    "    activation_records: List[Activation],\n",
    "    index: int,\n",
    "    repeat_non_zero_activations: bool = True,\n",
    "    numbered_list_of_n_explanations: int = None,\n",
    "    explanation: str = ''):\n",
    "    \n",
    "    message = f\"\"\"Neuron {index + 1}\n",
    "    Activations:{format_activation_records(activation_records, omit_zeros=False)}\"\"\"\n",
    "    \n",
    "    # We repeat the non-zero activations only if it was requested and if the proportion of\n",
    "    # non-zero activations isn't too high.\n",
    "    if (repeat_non_zero_activations) and (non_zero_activation_proportion(activation_records) < 0.2):\n",
    "        message += (\n",
    "            f\"\\nSame activations, but with all zeros filtered out:\"\n",
    "            f\"{format_activation_records(activation_records, omit_zeros=True)}\"\n",
    "        )\n",
    "        \n",
    "    # When set, this indicates that the prompt should solicit a numbered list of the given\n",
    "    # number of explanations, rather than a single explanation.\n",
    "    if numbered_list_of_n_explanations is None:\n",
    "        message += f\"\\nExplanation of neuron {index + 1} behavior:\"\n",
    "        message += f\" {MAX_ACT_DESCRIPTION_PREFIX}\"\n",
    "    \n",
    "    if explanation != '':\n",
    "        message += f\" {explanation}.\"\n",
    "        \n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b71d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ACT_FEW_SHOT_EXAMPLES: List[Example] = [example1, example2, example3]\n",
    "\n",
    "MAX_ACT_BASE_SYS_PROMPT = \"We're studying neurons in a neural network. Each neuron looks for some particular \" \\\n",
    "\"thing in a short document. Look at the parts of the document the neuron activates for \" \\\n",
    "\"and summarize in a single sentence what the neuron is looking for. Don't list \" \\\n",
    "\"examples of words.\\n\\nThe activation format is token<tab>activation. Activation \" \\\n",
    "\"values range from 0 to 10. A neuron finding what it's looking for is represented by a \" \\\n",
    "\"non-zero activation value. The higher the activation value, the stronger the match.\\n\"\n",
    "\n",
    "def generate_max_act_user_prompt(f: Feature, activating_examples: List[Activation] = None):\n",
    "    # for transluce we currently provide the max activating as parameter\n",
    "    max_activating = f.get_max_activating_examples() if activating_examples is None else activating_examples\n",
    "    user_prompt = add_per_neuron_explanation_prompt(activation_records=max_activating, index=0)\n",
    "    return user_prompt\n",
    "\n",
    "def build_sys_prompt():\n",
    "    prompt = MAX_ACT_BASE_SYS_PROMPT\n",
    "    for i, few_shot_example in enumerate(MAX_ACT_FEW_SHOT_EXAMPLES):\n",
    "        prompt += add_per_neuron_explanation_prompt(activation_records=few_shot_example.activation_records, index=i,\n",
    "                                                    explanation=few_shot_example.explanation)\n",
    "        prompt += \"\\n\"\n",
    "    return prompt\n",
    "\n",
    "MAX_ACT_SYS_PROMPT = build_sys_prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24da26de",
   "metadata": {},
   "source": [
    "### VocabProj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72ea1a8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "VOCAB_PROJ_SYS_PROMPT = \"You will be given a list of tokens related to a specific vector. These tokens represent a combination of embeddings that reconstruct the vector. Your task is to infer the most likely meaning or function of the vector based on these tokens. The list may include noise, such as unrelated terms, symbols, or programming jargon. Ignore whether the words are in multiple different languages, and do not mention it in your response. Focus on identifying a cohesive theme or concept shared by the most relevant tokens. Provide a specific sentence summarizing the meaning or function of the vector. Answer only with the summary. Avoid generic or overly broad answers, and disregard any noise in the list.\\nVector 1\\n    Tokens: ['contentLoaded', '▁hObject', ':✨', '▁AssemblyCulture', 'ContentAsync', '▁ivelany', '▁nahilalakip', 'IUrlHelper', '▁تضيفلها', '▁ErrIntOverflow'] ['▁could', 'could', '▁Could', 'Could', '▁COULD', '▁podría', '▁könnte', '▁podrían', '▁poderia', '▁könnten']\\nExplanation of vector 1 behavior: this vector is related to the word could.\\nVector 2\\n    Tokens: ['▁CreateTagHelper', '▁ldc', 'PropertyChanging', '▁jsPsych', 'ulement', '▁IBOutlet', '▁wireType', '▁initComponents', '▁متعلقه', 'Бахар'] ['▁مشين', '▁charity', '▁donation', '▁charitable', '▁volont', '▁donations', 'iNdEx', 'Parcelize', 'DatabaseError', 'BufferException']\\nExplanation of vector 2 behavior: this vector is related to charity and donations.\\nVector 3\\n    Tokens: ['▁tomorrow', '▁tonight', '▁yesterday', '▁today', 'yesterday', 'tomorrow', '▁demain', '▁Tomorrow', 'Tomorrow', '▁Yesterday'] ['▁Wex', 'ကိုးကား', 'Ārējās', 'piecze', ')$/,', '▁außer', '[]=$', 'cendental', 'ɜ', 'aderie']\\nExplanation of vector 3 behavior: this vector is related to specific dates, like tomorrow, tonight and yesterday.\\n\\n\"\n",
    "VOACB_PROJ_USER_PROMPT = \"Vector 4\\n    Tokens: {0}\\nExplanation of vector 4 behavior: this vector is related to\"\n",
    "\n",
    "def get_projection_data(m: Model, f: Feature, sae_w=None, encode=False, embed=False, k=50):\n",
    "    # project properly and get logits\n",
    "    if sae_w is None:\n",
    "        logits = m.m.unembed(m.m.ln_final(m.m.blocks[f.layer].mlp.W_out[f.feature]))\n",
    "    else:\n",
    "        feature_vector = sae_w[:, f.feature] if encode else sae_w[f.feature]\n",
    "        logits = (feature_vector @ m.m.embed.W_E.T) if embed else m.m.unembed(m.m.ln_final(feature_vector))\n",
    "        \n",
    "    topk = logits.topk(k)\n",
    "    bottomk = logits.topk(k, largest=False)\n",
    "    abs_topk = logits.abs().topk(k * 2)\n",
    "    top_tokens = m.m.to_str_tokens(topk.indices)\n",
    "    bottom_tokens = m.m.to_str_tokens(bottomk.indices)\n",
    "    top_abs_tokens = m.m.to_str_tokens(abs_topk.indices)\n",
    "    return (logits, topk, bottomk, abs_topk), (top_tokens, bottom_tokens, top_abs_tokens)\n",
    "\n",
    "def get_dec_unembed(m: Model, f: Feature, sae=None):\n",
    "    # sometimes these types don't match (e.g. llama's sae_w_dec is 'torch.bfloat16' while the model is 'torch.float32')\n",
    "    sae_w_dec = None\n",
    "    if sae is not None:\n",
    "        sae_w_dec = sae.W_dec\n",
    "        if sae.W_dec.dtype != m.m.W_U.dtype:\n",
    "            sae_w_dec = sae.W_dec.to(m.m.W_U.dtype)\n",
    "    \n",
    "    # the tokens we actually use - 'value' vectors of SAEs projected using the unembedding matrix #\n",
    "    _, tokens = get_projection_data(m, f, sae_w_dec)\n",
    "    (dec_top_tokens, dec_bottom_tokens, dec_top_abs_tokens) = tokens\n",
    "    \n",
    "    # take the dec unembed data\n",
    "    return (dec_top_tokens, dec_bottom_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41761016",
   "metadata": {},
   "source": [
    "### TokenChange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cc5811",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "TOKEN_CHANGE_SYS_PROMPT = \"You will be given a list of tokens related to a feature in an LLM. These tokens are the ones whose probabilities changed most when amplifying the feature. Your task is to infer the most likely meaning or function of the feature based on these tokens. The list may include noise, such as unrelated terms, symbols, or programming jargon. Provide a specific sentence summarizing the meaning or function of the feature. Answer only with the summary. Avoid generic or overly broad answers.\"\n",
    "\n",
    "def generate_token_change_user_prompt(real):\n",
    "    return str(list(set(real)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26e6c45",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def set_feature_act_hook(act, hook, feature, value):\n",
    "    act[:,:,feature] = value\n",
    "\n",
    "def get_intervention_tokens(model: HookedSAETransformer, prompts, f: Feature, value=200, sae=None):\n",
    "    if sae is None:  # transluce\n",
    "        clean_logits = model(prompts)\n",
    "        pos_inter_logits = model.run_with_hooks(prompts, fwd_hooks=[(f\"blocks.{f.layer}.mlp.hook_post\", \n",
    "                                                                     functools.partial(set_feature_act_hook, feature=f.feature, value=value))])\n",
    "        neg_inter_logits = model.run_with_hooks(prompts, fwd_hooks=[(f\"blocks.{f.layer}.mlp.hook_post\", \n",
    "                                                                     functools.partial(set_feature_act_hook, feature=f.feature, value=-value))])\n",
    "    else:\n",
    "        clean_logits = model.run_with_saes(prompts, saes=[sae])\n",
    "        pos_inter_logits = model.run_with_hooks_with_saes(prompts, saes=[sae], fwd_hooks=[(f\"{sae.cfg.hook_name}.hook_sae_acts_post\", \n",
    "                                                                                       functools.partial(set_feature_act_hook, feature=f.feature, value=value))])\n",
    "        neg_inter_logits = model.run_with_hooks_with_saes(prompts, saes=[sae], fwd_hooks=[(f\"{sae.cfg.hook_name}.hook_sae_acts_post\", \n",
    "                                                                                       functools.partial(set_feature_act_hook, feature=f.feature, value=-value))])\n",
    "        \n",
    "    pos_diff_logits = (pos_inter_logits - clean_logits).mean(dim=0).mean(dim=0)\n",
    "    neg_diff_logits = (neg_inter_logits - clean_logits).mean(dim=0).mean(dim=0)\n",
    "\n",
    "    neg_toks = model.to_str_tokens(pos_diff_logits.topk(10).indices) + model.to_str_tokens(neg_diff_logits.topk(10, largest=False).indices)\n",
    "    pos_toks = model.to_str_tokens(pos_diff_logits.topk(10, largest=False).indices) + model.to_str_tokens(neg_diff_logits.topk(10).indices)\n",
    "\n",
    "    return pos_toks + neg_toks\n",
    "\n",
    "def get_causal_data(m: Model, f: Feature, sae=None):\n",
    "    pile = m.get_pile_dataset()\n",
    "    pile_sample = pile[torch.randint(0, len(pile), (32,))].to(device)\n",
    "    real = get_intervention_tokens(m.m, pile_sample, f, value=10, sae=sae)\n",
    "    return real, None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90743e0e",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da49f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENSEMBLE_RAW_VM_SYS_PROMPT = \"\"\"We're studying neurons in a neural network. Each neuron has certain inputs that activate it and outputs that it leads to. You will receive two pieces of information about a neuron: the activations it has for certain inputs, the words its output is most associated with. These will be separated into two sections [INPUT] and [OUTPUT].\n",
    "\n",
    "The [INPUT] activation format is token<tab>activation. Activation values range from 0 to 10. A neuron finding what it's looking for is represented by a non-zero activation value. The higher the activation value, the stronger the match.\n",
    "\n",
    "The [OUTPUT] format is a list of words related to that specific neuron. These tokens represent a combination of embeddings that reconstruct the vector. You can infer the most likely output or function of the neuron based on these tokens. The list may include noise, such as unrelated terms, symbols, or programming jargon. Ignore whether the words are in multiple different languages, and do not mention it in your response. Focus on identifying a cohesive theme or concept shared by the most relevant tokens.\n",
    "\n",
    "Your response should be a concise (1-2 sentence) explanation of the neuron, encompassing what triggers it (input) and what it does once triggered (output). If the two sides relate to one another you may include that in your explanation, otherwise simply state the input and output.\n",
    "\n",
    "Neuron 1\n",
    "\n",
    "[INPUT]\n",
    "    Activations:\n",
    "<start>\n",
    "t\t0\n",
    "urt\t0\n",
    "ur\t0\n",
    "ro\t0\n",
    " is\t0\n",
    " fab\t0\n",
    "ulously\t0\n",
    " funny\t0\n",
    " and\t0\n",
    " over\t0\n",
    " the\t0\n",
    " top\t0\n",
    " as\t0\n",
    " a\t0\n",
    " '\t0\n",
    "very\t0\n",
    " sneaky\t0\n",
    "'\t1\n",
    " but\t0\n",
    "ler\t0\n",
    " who\t0\n",
    " excel\t0\n",
    "s\t0\n",
    " in\t0\n",
    " the\t0\n",
    " art\t0\n",
    " of\t0\n",
    " impossible\t0\n",
    " disappearing\t6\n",
    "/\t0\n",
    "re\t0\n",
    "app\t0\n",
    "earing\t10\n",
    " acts\t0\n",
    "<end>\n",
    "<start>\n",
    "esc\t0\n",
    "aping\t10\n",
    " the\t4\n",
    " studio\t0\n",
    " ,\t0\n",
    " pic\t0\n",
    "col\t0\n",
    "i\t0\n",
    " is\t0\n",
    " warm\t0\n",
    "ly\t0\n",
    " affecting\t3\n",
    " and\t0\n",
    " so\t0\n",
    " is\t0\n",
    " this\t0\n",
    " ad\t0\n",
    "roit\t0\n",
    "ly\t0\n",
    " minimalist\t0\n",
    " movie\t0\n",
    " .\t0\n",
    "<end>\n",
    "\n",
    "Same activations, but with all zeros filtered out:\n",
    "<start>\n",
    "'\t1\n",
    " disappearing\t6\n",
    "earing\t10\n",
    "<end>\n",
    "<start>\n",
    "aping\t10\n",
    " the\t4\n",
    " affecting\t3\n",
    "<end>\n",
    "\n",
    "[OUTPUT]\n",
    "['to', 'To', 'TO', 'Towards', 'towards', 'TOWARDS', 'toward', 'Toward', 'TOWARD', 'toward', 'Toward', 'TOWARD', 'life', 'do', 'fdsa', 'aaaaaa', 'aaaaa', 'aaaa', 'aaa', 'aa', 'a', 'A']\n",
    "\n",
    "Explanation of neuron 1 behavior: the main thing this neuron does is find present tense verbs ending in 'ing', and then outputs words related to directionality or movement to or towards something.\n",
    "\n",
    "Neuron 2\n",
    "\n",
    "[INPUT]\n",
    "    Activations:\n",
    "<start>\n",
    "as\t0\n",
    " sac\t0\n",
    "char\t0\n",
    "ine\t0\n",
    " movies\t0\n",
    " go\t0\n",
    " ,\t0\n",
    " this\t0\n",
    " is\t0\n",
    " likely\t0\n",
    " to\t0\n",
    " cause\t0\n",
    " massive\t0\n",
    " cardiac\t0\n",
    " arrest\t10\n",
    " if\t0\n",
    " taken\t0\n",
    " in\t0\n",
    " large\t0\n",
    " doses\t0\n",
    " .\t0\n",
    "<end>\n",
    "<start>\n",
    "shot\t0\n",
    " perhaps\t0\n",
    "'\t0\n",
    "art\t0\n",
    "istically\t0\n",
    "'\t0\n",
    " with\t0\n",
    "handheld\t0\n",
    " cameras\t0\n",
    " and\t0\n",
    " apparently\t0\n",
    " no\t0\n",
    " movie\t0\n",
    " lights\t0\n",
    " by\t0\n",
    " jo\t0\n",
    "aquin\t0\n",
    " b\t0\n",
    "aca\t0\n",
    "-\t0\n",
    "as\t0\n",
    "ay\t0\n",
    " ,\t0\n",
    " the\t0\n",
    " low\t0\n",
    "-\t0\n",
    "budget\t0\n",
    " production\t0\n",
    " swings\t0\n",
    " annoy\t0\n",
    "ingly\t0\n",
    " between\t0\n",
    " vert\t0\n",
    "igo\t10\n",
    " and\t0\n",
    " opacity\t0\n",
    " .\t0\n",
    "<end>\n",
    "\n",
    "Same activations, but with all zeros filtered out:\n",
    "<start>\n",
    " arrest\t10\n",
    "<end>\n",
    "<start>\n",
    "igo\t10\n",
    "<end>\n",
    "\n",
    "[OUTPUT]\n",
    "['1111', 'Evol', 'crab', 'sing', 'dance', 'walk', 'run', 'jump', 'swim', 'climb', 'death', 'Death', 'DEATH', 'dying', 'Dying', 'DYING', 'die', 'DIED', 'kill']\n",
    "\n",
    "Explanation of neuron 2 behavior: the main thing this neuron does is find words related to physical medical conditions, and then outputs words related to death or dying.\n",
    "\n",
    "Neuron 3\n",
    "\n",
    "[INPUT]\n",
    "    Activations:\n",
    "<start>\n",
    "the\t0\n",
    " sense\t0\n",
    " of\t0\n",
    " together\t5\n",
    "ness\t10\n",
    " in\t0\n",
    " our\t1\n",
    " town\t2\n",
    " is\t0\n",
    " strong\t0\n",
    " .\t0\n",
    "<end>\n",
    "<start>\n",
    "a\t0\n",
    " buoy\t0\n",
    "ant\t0\n",
    " romantic\t0\n",
    " comedy\t0\n",
    " about\t0\n",
    " friendship\t0\n",
    ",\t0\n",
    " love\t0\n",
    ",\t0\n",
    " and\t0\n",
    " the\t0\n",
    " truth\t0\n",
    " that\t0\n",
    " we\t2\n",
    "'re\t4\n",
    " all\t3\n",
    " in\t7\n",
    " this\t10\n",
    " together\t5\n",
    " .\t0\n",
    "<end>\n",
    "\n",
    "[OUTPUT]\n",
    "['community', 'commune', 'communal', 'Community', 'family', 'Family', 'Together', 'ball', 'street', 'efu', 'jefus']\n",
    "\n",
    "Explanation of neuron 3 behavior: the main thing this neuron does is find phrases related to community, and then outputs words related to togetherness or family.\n",
    "\"\"\"\n",
    "\n",
    "ENSEMBLE_RAW_TM_SYS_PROMPT = \"\"\"We're studying neurons in a neural network. Each neuron has certain inputs that activate it and outputs that it leads to. You will receive two pieces of information about a neuron: the activations it has for certain inputs, the words its output is most associated with. These will be separated into two sections [INPUT] and [OUTPUT].\n",
    "\n",
    "The [INPUT] activation format is token<tab>activation. Activation values range from 0 to 10. A neuron finding what it's looking for is represented by a non-zero activation value. The higher the activation value, the stronger the match.\n",
    "\n",
    "The [OUTPUT] format is a list of the words whose probabilities changed most when amplifying that specific neuron. The list may include noise, such as unrelated terms, symbols, or programming jargon. Focus on identifying a cohesive theme or concept shared by the most relevant tokens.\n",
    "\n",
    "Your response should be a concise (1-2 sentence) explanation of the neuron, encompassing what triggers it (input) and what it does once triggered (output). If the two sides relate to one another you may include that in your explanation, otherwise simply state the input and output.\n",
    "\n",
    "Neuron 1\n",
    "\n",
    "[INPUT]\n",
    "    Activations:\n",
    "<start>\n",
    "t\t0\n",
    "urt\t0\n",
    "ur\t0\n",
    "ro\t0\n",
    " is\t0\n",
    " fab\t0\n",
    "ulously\t0\n",
    " funny\t0\n",
    " and\t0\n",
    " over\t0\n",
    " the\t0\n",
    " top\t0\n",
    " as\t0\n",
    " a\t0\n",
    " '\t0\n",
    "very\t0\n",
    " sneaky\t0\n",
    "'\t1\n",
    " but\t0\n",
    "ler\t0\n",
    " who\t0\n",
    " excel\t0\n",
    "s\t0\n",
    " in\t0\n",
    " the\t0\n",
    " art\t0\n",
    " of\t0\n",
    " impossible\t0\n",
    " disappearing\t6\n",
    "/\t0\n",
    "re\t0\n",
    "app\t0\n",
    "earing\t10\n",
    " acts\t0\n",
    "<end>\n",
    "<start>\n",
    "esc\t0\n",
    "aping\t10\n",
    " the\t4\n",
    " studio\t0\n",
    " ,\t0\n",
    " pic\t0\n",
    "col\t0\n",
    "i\t0\n",
    " is\t0\n",
    " warm\t0\n",
    "ly\t0\n",
    " affecting\t3\n",
    " and\t0\n",
    " so\t0\n",
    " is\t0\n",
    " this\t0\n",
    " ad\t0\n",
    "roit\t0\n",
    "ly\t0\n",
    " minimalist\t0\n",
    " movie\t0\n",
    " .\t0\n",
    "<end>\n",
    "\n",
    "Same activations, but with all zeros filtered out:\n",
    "<start>\n",
    "'\t1\n",
    " disappearing\t6\n",
    "earing\t10\n",
    "<end>\n",
    "<start>\n",
    "aping\t10\n",
    " the\t4\n",
    " affecting\t3\n",
    "<end>\n",
    "\n",
    "[OUTPUT]\n",
    "['to', 'To', 'TO', 'Towards', 'towards', 'TOWARDS', 'toward', 'Toward', 'TOWARD', 'toward', 'Toward', 'TOWARD', 'life', 'do', 'fdsa', 'aaaaaa', 'aaaaa', 'aaaa', 'aaa', 'aa', 'a', 'A']\n",
    "\n",
    "Explanation of neuron 1 behavior: the main thing this neuron does is find present tense verbs ending in 'ing', and then outputs words related to directionality or movement to or towards something.\n",
    "\n",
    "Neuron 2\n",
    "\n",
    "[INPUT]\n",
    "    Activations:\n",
    "<start>\n",
    "as\t0\n",
    " sac\t0\n",
    "char\t0\n",
    "ine\t0\n",
    " movies\t0\n",
    " go\t0\n",
    " ,\t0\n",
    " this\t0\n",
    " is\t0\n",
    " likely\t0\n",
    " to\t0\n",
    " cause\t0\n",
    " massive\t0\n",
    " cardiac\t0\n",
    " arrest\t10\n",
    " if\t0\n",
    " taken\t0\n",
    " in\t0\n",
    " large\t0\n",
    " doses\t0\n",
    " .\t0\n",
    "<end>\n",
    "<start>\n",
    "shot\t0\n",
    " perhaps\t0\n",
    "'\t0\n",
    "art\t0\n",
    "istically\t0\n",
    "'\t0\n",
    " with\t0\n",
    "handheld\t0\n",
    " cameras\t0\n",
    " and\t0\n",
    " apparently\t0\n",
    " no\t0\n",
    " movie\t0\n",
    " lights\t0\n",
    " by\t0\n",
    " jo\t0\n",
    "aquin\t0\n",
    " b\t0\n",
    "aca\t0\n",
    "-\t0\n",
    "as\t0\n",
    "ay\t0\n",
    " ,\t0\n",
    " the\t0\n",
    " low\t0\n",
    "-\t0\n",
    "budget\t0\n",
    " production\t0\n",
    " swings\t0\n",
    " annoy\t0\n",
    "ingly\t0\n",
    " between\t0\n",
    " vert\t0\n",
    "igo\t10\n",
    " and\t0\n",
    " opacity\t0\n",
    " .\t0\n",
    "<end>\n",
    "\n",
    "Same activations, but with all zeros filtered out:\n",
    "<start>\n",
    " arrest\t10\n",
    "<end>\n",
    "<start>\n",
    "igo\t10\n",
    "<end>\n",
    "\n",
    "[OUTPUT]\n",
    "['1111', 'Evol', 'crab', 'sing', 'dance', 'walk', 'run', 'jump', 'swim', 'climb', 'death', 'Death', 'DEATH', 'dying', 'Dying', 'DYING', 'die', 'DIED', 'kill']\n",
    "\n",
    "Explanation of neuron 2 behavior: the main thing this neuron does is find words related to physical medical conditions, and then outputs words related to death or dying.\n",
    "\n",
    "Neuron 3\n",
    "\n",
    "[INPUT]\n",
    "    Activations:\n",
    "<start>\n",
    "the\t0\n",
    " sense\t0\n",
    " of\t0\n",
    " together\t5\n",
    "ness\t10\n",
    " in\t0\n",
    " our\t1\n",
    " town\t2\n",
    " is\t0\n",
    " strong\t0\n",
    " .\t0\n",
    "<end>\n",
    "<start>\n",
    "a\t0\n",
    " buoy\t0\n",
    "ant\t0\n",
    " romantic\t0\n",
    " comedy\t0\n",
    " about\t0\n",
    " friendship\t0\n",
    ",\t0\n",
    " love\t0\n",
    ",\t0\n",
    " and\t0\n",
    " the\t0\n",
    " truth\t0\n",
    " that\t0\n",
    " we\t2\n",
    "'re\t4\n",
    " all\t3\n",
    " in\t7\n",
    " this\t10\n",
    " together\t5\n",
    " .\t0\n",
    "<end>\n",
    "\n",
    "[OUTPUT]\n",
    "['community', 'commune', 'communal', 'Community', 'family', 'Family', 'Together', 'ball', 'street', 'efu', 'jefus']\n",
    "\n",
    "Explanation of neuron 3 behavior: the main thing this neuron does is find phrases related to community, and then outputs words related to togetherness or family.\n",
    "\"\"\"\n",
    "\n",
    "ENSEMBLE_RAW_ALL_SYS_PROMPT = \"\"\"We're studying neurons in a neural network. Each neuron has certain inputs that activate it and outputs that it leads to. You will receive two pieces of information about a neuron: the activations it has for certain inputs, the words its output is most associated with. These will be separated into two sections [INPUT] and [OUTPUT].\n",
    "\n",
    "The [INPUT] activation format is token<tab>activation. Activation values range from 0 to 10. A neuron finding what it's looking for is represented by a non-zero activation value. The higher the activation value, the stronger the match.\n",
    "\n",
    "The [OUTPUT] format is a list of words related to that specific neuron. These tokens are in two lists, one represents a combination of embeddings that reconstruct the vector, and the other is the tokens most affected by amplifying that neuron. You can infer the most likely output or function of the neuron based on these tokens. The list may include noise, such as unrelated terms, symbols, or programming jargon. Ignore whether the words are in multiple different languages, and do not mention it in your response. Focus on identifying a cohesive theme or concept shared by the most relevant tokens.\n",
    "\n",
    "Your response should be a concise (1-2 sentence) explanation of the neuron, encompassing what triggers it (input) and what it does once triggered (output). If the two sides relate to one another you may include that in your explanation, otherwise simply state the input and output.\n",
    "\n",
    "Neuron 1\n",
    "\n",
    "[INPUT]\n",
    "    Activations:\n",
    "<start>\n",
    "t\t0\n",
    "urt\t0\n",
    "ur\t0\n",
    "ro\t0\n",
    " is\t0\n",
    " fab\t0\n",
    "ulously\t0\n",
    " funny\t0\n",
    " and\t0\n",
    " over\t0\n",
    " the\t0\n",
    " top\t0\n",
    " as\t0\n",
    " a\t0\n",
    " '\t0\n",
    "very\t0\n",
    " sneaky\t0\n",
    "'\t1\n",
    " but\t0\n",
    "ler\t0\n",
    " who\t0\n",
    " excel\t0\n",
    "s\t0\n",
    " in\t0\n",
    " the\t0\n",
    " art\t0\n",
    " of\t0\n",
    " impossible\t0\n",
    " disappearing\t6\n",
    "/\t0\n",
    "re\t0\n",
    "app\t0\n",
    "earing\t10\n",
    " acts\t0\n",
    "<end>\n",
    "<start>\n",
    "esc\t0\n",
    "aping\t10\n",
    " the\t4\n",
    " studio\t0\n",
    " ,\t0\n",
    " pic\t0\n",
    "col\t0\n",
    "i\t0\n",
    " is\t0\n",
    " warm\t0\n",
    "ly\t0\n",
    " affecting\t3\n",
    " and\t0\n",
    " so\t0\n",
    " is\t0\n",
    " this\t0\n",
    " ad\t0\n",
    "roit\t0\n",
    "ly\t0\n",
    " minimalist\t0\n",
    " movie\t0\n",
    " .\t0\n",
    "<end>\n",
    "\n",
    "Same activations, but with all zeros filtered out:\n",
    "<start>\n",
    "'\t1\n",
    " disappearing\t6\n",
    "earing\t10\n",
    "<end>\n",
    "<start>\n",
    "aping\t10\n",
    " the\t4\n",
    " affecting\t3\n",
    "<end>\n",
    "\n",
    "[OUTPUT]\n",
    "['to', 'To', 'TO', 'Towards', 'towards', 'TOWARDS', 'toward', 'Toward', 'TOWARD', 'toward', 'Toward', 'TOWARD', 'life', 'do', 'fdsa', 'aaaaaa', 'aaaaa', 'aaaa', 'aaa', 'aa', 'a', 'A']\n",
    "['thought', 'think', 'Think', 'to', 'towards', 'Towar', 'towards', 'toward', 'Toward', 'TOWARD', 'life', 'do', 'fdsa', 'aaaaaa', 'aaaaa', 'aaaa', 'aaa', 'aa', 'a', 'A']\n",
    "\n",
    "Explanation of neuron 1 behavior: the main thing this neuron does is find present tense verbs ending in 'ing', and then outputs words related to directionality, movement or thinking.\n",
    "\n",
    "Neuron 2\n",
    "\n",
    "[INPUT]\n",
    "    Activations:\n",
    "<start>\n",
    "as\t0\n",
    " sac\t0\n",
    "char\t0\n",
    "ine\t0\n",
    " movies\t0\n",
    " go\t0\n",
    " ,\t0\n",
    " this\t0\n",
    " is\t0\n",
    " likely\t0\n",
    " to\t0\n",
    " cause\t0\n",
    " massive\t0\n",
    " cardiac\t0\n",
    " arrest\t10\n",
    " if\t0\n",
    " taken\t0\n",
    " in\t0\n",
    " large\t0\n",
    " doses\t0\n",
    " .\t0\n",
    "<end>\n",
    "<start>\n",
    "shot\t0\n",
    " perhaps\t0\n",
    "'\t0\n",
    "art\t0\n",
    "istically\t0\n",
    "'\t0\n",
    " with\t0\n",
    "handheld\t0\n",
    " cameras\t0\n",
    " and\t0\n",
    " apparently\t0\n",
    " no\t0\n",
    " movie\t0\n",
    " lights\t0\n",
    " by\t0\n",
    " jo\t0\n",
    "aquin\t0\n",
    " b\t0\n",
    "aca\t0\n",
    "-\t0\n",
    "as\t0\n",
    "ay\t0\n",
    " ,\t0\n",
    " the\t0\n",
    " low\t0\n",
    "-\t0\n",
    "budget\t0\n",
    " production\t0\n",
    " swings\t0\n",
    " annoy\t0\n",
    "ingly\t0\n",
    " between\t0\n",
    " vert\t0\n",
    "igo\t10\n",
    " and\t0\n",
    " opacity\t0\n",
    " .\t0\n",
    "<end>\n",
    "\n",
    "Same activations, but with all zeros filtered out:\n",
    "<start>\n",
    " arrest\t10\n",
    "<end>\n",
    "<start>\n",
    "igo\t10\n",
    "<end>\n",
    "\n",
    "[OUTPUT]\n",
    "['1111', 'Evol', 'crab', 'sing', 'dance', 'walk', 'run', 'jump', 'swim', 'climb', 'death', 'Death', 'DEATH', 'dying', 'Dying', 'DYING', 'die', 'DIED', 'kill']\n",
    "['die', 'morgue', 'Die', 'murder', 'dance', 'Dancing', 'Dancer', 'walk', 'run', 'jump']\n",
    "\n",
    "Explanation of neuron 2 behavior: the main thing this neuron does is find words related to physical medical conditions, and then outputs words related to death or dying, as well as physical movement.\n",
    "\n",
    "Neuron 3\n",
    "\n",
    "[INPUT]\n",
    "    Activations:\n",
    "<start>\n",
    "the\t0\n",
    " sense\t0\n",
    " of\t0\n",
    " together\t5\n",
    "ness\t10\n",
    " in\t0\n",
    " our\t1\n",
    " town\t2\n",
    " is\t0\n",
    " strong\t0\n",
    " .\t0\n",
    "<end>\n",
    "<start>\n",
    "a\t0\n",
    " buoy\t0\n",
    "ant\t0\n",
    " romantic\t0\n",
    " comedy\t0\n",
    " about\t0\n",
    " friendship\t0\n",
    ",\t0\n",
    " love\t0\n",
    ",\t0\n",
    " and\t0\n",
    " the\t0\n",
    " truth\t0\n",
    " that\t0\n",
    " we\t2\n",
    "'re\t4\n",
    " all\t3\n",
    " in\t7\n",
    " this\t10\n",
    " together\t5\n",
    " .\t0\n",
    "<end>\n",
    "\n",
    "[OUTPUT]\n",
    "['community', 'commune', 'communal', 'Community', 'family', 'Family', 'Together', 'ball', 'street', 'efu', 'jefus']\n",
    "['do', 'I', 'what', go', 'commune', 'communal', 'Community', 'family', 'Family', 'Together', 'ball', 'street', 'efu', 'jefus']\n",
    "\n",
    "Explanation of neuron 3 behavior: the main thing this neuron does is find phrases related to community, and then outputs words related to togetherness or family.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc679f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENSEMBLE_IO_USER_PROMPT = \"\"\"Neuron 4\n",
    "[INPUT]\n",
    "{input_data}\n",
    "\n",
    "[OUTPUT]\n",
    "{output_data}\n",
    "\n",
    "Explanation of neuron 4 behavior:\"\"\"\n",
    "\n",
    "ENSEMBLE_IOO_USER_PROMPT = \"\"\"Neuron 4\n",
    "\n",
    "[INPUT]\n",
    "{input_data}\n",
    "\n",
    "[OUTPUT]\n",
    "{output_data1}\n",
    "{output_data2}\n",
    "\n",
    "Explanation of neuron 4 behavior:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cfa07a",
   "metadata": {},
   "source": [
    "## Description Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d7b546",
   "metadata": {},
   "source": [
    "### Input Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9da7b3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "GEN_LISTS_PROMPT = \"I'm going to give you explanations and interpretations of features from LLMs. You must take in each expalantion, and generate 5 sentences for which you think the feature will have a high activation, and 5 for which they'll have a low activation. For the high activation, make sure to choose ones that will cause a high activation with high confidence - you don't have to include all groups, just make examples that you're confident will have high activation. Make the sentences both include the words from the explanation, and represent the concept. Try to use specific examples, and make them literal interpretations of the explanation, without trying to generalize. Low activation sentences should have nothing to do with the interpretation - i.e. they should by orthogonal and completely unrelated. Please output the response in json format with a 'positive' key and a 'negative' key. Output only the json and no other explanation. Make sure the json is formatted correctly - do not include any '`' backtick characters characters, i.e. do not format as code, just return the json text. The explanations should be five and five overall, not per line.\\n\\n{explanation}\\n\"\n",
    "\n",
    "FIX_JSON_PROMPT = \"\"\"Please fix this json that is not formatted correctly. Write only the fixed json. \n",
    "DO NOT write anything but the json. No comments. Only the json itself.\\n\\n{json}\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727db473",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_lists(explanation):\n",
    "    \"\"\"\n",
    "    Generates 5 positive and negative activating examples based on our explanation\n",
    "    \"\"\"\n",
    "    return gemini_explainer([{\"role\": \"user\", \"content\": GEN_LISTS_PROMPT.format(explanation=explanation)}])\n",
    "\n",
    "def get_pos_neg(lists):\n",
    "    try:\n",
    "        json_content = lists.strip().strip(\"json\").strip('`').removeprefix('json\\n').removesuffix('\\n').strip().strip(\"json\").strip(\"`\").strip('`').removeprefix('json\\n').removesuffix('\\n')\n",
    "        j = json.loads(json_content)\n",
    "        return j[\"positive\"], j[\"negative\"]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for top in range(3):\n",
    "        for bottom in range(2):\n",
    "            bottom = None if bottom == 0 else -bottom\n",
    "            new_lists = lists.splitlines()[top:bottom]\n",
    "\n",
    "            try:\n",
    "                j = json.loads(new_lists)\n",
    "                return j[\"positive\"], j[\"negative\"]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    lists = gemini_explainer([{\"role\": \"user\", \"content\": FIX_JSON_PROMPT.format(json=lists)}])\n",
    "    try:\n",
    "        json_content = lists.strip().strip(\"json\").strip('`').removeprefix('json\\n').removesuffix('\\n').strip().strip(\"json\").strip(\"`\").strip('`').removeprefix('json\\n').removesuffix('\\n')\n",
    "        j = json.loads(json_content)\n",
    "        return j[\"positive\"], j[\"negative\"]\n",
    "    except:\n",
    "        logger.error(lists)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccba1fca",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_pos_neg_acts(model: HookedSAETransformer, pos, neg, f: Feature, pre_relu=False, sae=None):\n",
    "    if sae is None:  # transluce\n",
    "        pos_cache = model.run_with_cache(pos, return_type=None)[1]\n",
    "        neg_cache = model.run_with_cache(neg, return_type=None)[1]\n",
    "    else:\n",
    "        pos_cache = model.run_with_cache_with_saes(pos, saes=[sae], return_type=None)[1]\n",
    "        neg_cache = model.run_with_cache_with_saes(neg, saes=[sae], return_type=None)[1]\n",
    "    \n",
    "    relu = \"pre\" if pre_relu else \"post\"\n",
    "\n",
    "    # we take the maximal activation of the feature across all sentences across all tokens\n",
    "    # we prefer that because we can generate multiple exps per feature and then take the best activations across all exps\n",
    "    if sae is None:  # transluce\n",
    "        block = f\"blocks.{f.layer}.mlp.hook_post\"\n",
    "    else:\n",
    "        block = f\"{sae.cfg.hook_name}.hook_sae_acts_{relu}\"\n",
    "        \n",
    "    pos_act_max_all = pos_cache[block][:, :, f.feature].max().item()\n",
    "    neg_act_max_all = neg_cache[block][:, :, f.feature].max().item()\n",
    "    pos_act_max_toks = pos_cache[block][:, :, f.feature].max(dim=-1).values.mean().item()\n",
    "    neg_act_max_toks = neg_cache[block][:, :, f.feature].max(dim=-1).values.mean().item()\n",
    "\n",
    "    return pos_act_max_all, neg_act_max_all, pos_act_max_toks, neg_act_max_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13efd3d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class InputScore:\n",
    "    pos_act_all: float\n",
    "    neg_act_all: float\n",
    "    pos_act_toks: float\n",
    "    neg_act_toks: float\n",
    "    pos_list: float\n",
    "    neg_list: float\n",
    "    \n",
    "    def success(self) -> bool:\n",
    "        # the input metric final test for success\n",
    "        return self.pos_act_toks > self.neg_act_toks\n",
    "    \n",
    "    @classmethod\n",
    "    def from_row(cls, row):\n",
    "        return cls(**vars(row))\n",
    "    \n",
    "\n",
    "def get_input_score(description: str, m: Model, f: Feature, sae=None) -> InputScore:\n",
    "    lists = get_lists(description)    \n",
    "    pos_list, neg_list = get_pos_neg(lists)\n",
    "    pos_act_all, neg_act_all, pos_act_toks, neg_act_toks = get_pos_neg_acts(m.m, pos_list, neg_list, f, sae=sae)\n",
    "    return InputScore(pos_act_all, neg_act_all, pos_act_toks, neg_act_toks, pos_list, neg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c6cac9",
   "metadata": {},
   "source": [
    "### Output Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79091965",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "OUTPUT_METRIC_GENERATION_PROMPTS = [\"The explanation is simple:\", \"I think\", \"We\"]\n",
    "\n",
    "OUTPUT_METRIC_SYS_PROMPT = \"\"\"You are analyzing the behavior of a specific neuron in a language model. You will receive:\n",
    "\n",
    "1. A hypothesized explanation for what concept the neuron represents (e.g., specific tokens, themes, or ideas).\n",
    "2. Three sets of completions, one generated by amplifying the activation of the neuron in question, and one of a random neuron across the same prompts.\n",
    "\n",
    "Your goal is to identify which set of completions is more likely the result of amplifying the neuron in question. To do this:\n",
    "- Look for completions where the **literal words** or the **ideas/themes** described in the explanation occur more frequently or with greater emphasis.\n",
    "- Remember that amplification may highlight specific words or their broader contextual meanings, meaning that a lot of the times they might be very noisy, but contain keywords that appear in the explanation.\n",
    "- Your answer should be based on the **content** of the completions, not the quality of the language model's output.\n",
    "- Your reasoning should be sound, don't make overly elaborate and far-fetched connections.\n",
    "\n",
    "The first line in your response should be a brief explanation of your choice - what made you choose that set of completions.\n",
    "The second line must be only the set number you think matches the description (i.e., 1, 2 or 3) and no other text. You must pick one of the three sets.\n",
    "\"\"\"\n",
    "\n",
    "STEERING_AMP_PROMPT_TEMPLATE = \"\"\"<{amplification}> {completions}\\n\"\"\"\n",
    "\n",
    "STEERING_FULL_PROMPT_TEMPLATE = \"\"\"Explanation: {explanation}\n",
    "\n",
    "# Set 1\n",
    "{amplifications1}\n",
    "\n",
    "# Set 2\n",
    "{amplifications2}\n",
    "\n",
    "# Set 3\n",
    "{amplifications3}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2014529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_feature_act_kl_hook(act, hook, feature: int, value):\n",
    "    act[:,:,feature] = value\n",
    "\n",
    "def kl_div(p, q, eps=1e-10):\n",
    "    \"\"\"\n",
    "    Get the KL Divergence between p and q\n",
    "    \"\"\"\n",
    "    p = p.clamp(min=eps)\n",
    "    q = q.clamp(min=eps)\n",
    "    return torch.sum(p * (torch.log(p) - torch.log(q)), dim=-1)\n",
    "\n",
    "def get_kl_div(model: Model, prompts, f: Feature, value, sae=None):\n",
    "    \"\"\"\n",
    "    Get KL Divergence between clean and hooked activations when clamping the feature to value, \n",
    "    averaged over all of the prompts\n",
    "    \"\"\"\n",
    "    toks = model.m.to_tokens(prompts)\n",
    "\n",
    "    if sae is None:\n",
    "        clean_probs = model.m(toks)\n",
    "        hooked_probs = model.m.run_with_hooks(toks, fwd_hooks=[(f\"blocks.{f.layer}.mlp.hook_pre\", \n",
    "                                                              functools.partial(set_feature_act_kl_hook, feature=f.feature, value=value))]).softmax(dim=-1)\n",
    "    else:\n",
    "        clean_probs = model.m.run_with_saes(toks, saes=[sae])\n",
    "        hooked_probs = model.m.run_with_hooks_with_saes(toks, saes=[sae], fwd_hooks=[(f\"{sae.cfg.hook_name}.hook_sae_acts_post\", \n",
    "                                                                                  functools.partial(set_feature_act_kl_hook, feature=f.feature, value=value))]).softmax(dim=-1)\n",
    "    clean_probs = clean_probs.softmax(dim=-1)\n",
    "\n",
    "    # Remove logits which are padding tokens\n",
    "    clean_probs[toks == 0] = 0\n",
    "    hooked_probs[toks == 0] = 0\n",
    "\n",
    "    # return hooked_probs, clean_probs\n",
    "    kl = kl_div(clean_probs, hooked_probs)\n",
    "    means = []\n",
    "    for row in kl:\n",
    "        means.append(row[row != 0].mean().item())\n",
    "\n",
    "    return np.mean(means)\n",
    "\n",
    "\n",
    "def get_activation_for_kl(model: Model, prompts, f: Feature, target_kl, high_thresh=0.1, neg=False, verbose=False, sae=None):\n",
    "    \"\"\"\n",
    "    Find the activation value we need for the desired target KL Divergence value\n",
    "    \"\"\"\n",
    "    # Do binary search between 0 and 1000\n",
    "    low, high = (-1000, -1) if neg else (1, 1000)\n",
    "    kl = -1\n",
    "    mid = 0\n",
    "    while (low+1 < high) and (kl < target_kl or kl > target_kl + high_thresh):\n",
    "        mid = (low + high) // 2\n",
    "        kl = get_kl_div(model, prompts, f, mid, sae=sae)\n",
    "\n",
    "        if (neg and kl < target_kl) or (not neg and kl > target_kl):\n",
    "            high = mid\n",
    "        else:\n",
    "            low = mid\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Low: {low}, High: {high}, Mid: {mid}, KL: {kl}, Target KL: {target_kl}\")\n",
    "\n",
    "    return mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4e110d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def gen_hook(clean_act, hook, feature: int, value, sae=None):\n",
    "    \"\"\"\n",
    "    Manually steers the value inside an SAE activation using the basic activation\n",
    "    :param clean_act: the basic activation before the SAE\n",
    "    \"\"\"\n",
    "    if sae is None:  # transluce\n",
    "        clean_act[:,:,feature] = value\n",
    "        return clean_act\n",
    "    \n",
    "    encoded_act = sae.encode(clean_act)\n",
    "    dirty_act = sae.decode(encoded_act)\n",
    "    error_term = clean_act - dirty_act\n",
    "\n",
    "    encoded_act[:,:,feature] = value\n",
    "    hooked_act = sae.decode(encoded_act) + error_term\n",
    "\n",
    "    return hooked_act\n",
    "    \n",
    "def hooked_gen(prompt, model: Model, f: Feature, n=25, value=10, verbose=False, temperature=1, sae=None):\n",
    "    \"\"\"\n",
    "    :param prompt: the prompt for which the model will generate text\n",
    "    :param value: the value to amplify the feature with\n",
    "    \"\"\"\n",
    "    model.m.reset_hooks()\n",
    "    block = f\"blocks.{f.layer}.mlp.hook_pre\" if sae is None else sae.cfg.hook_name\n",
    "    model.m.add_hook(block, functools.partial(gen_hook, feature=f.feature, value=value, sae=sae))\n",
    "    output = model.m.generate(prompt, max_new_tokens=n, verbose=verbose, temperature=temperature)\n",
    "    model.m.reset_hooks()\n",
    "\n",
    "    return [x[len(model.m.to_string(prompt[i])):] for i, x in enumerate(model.m.to_string(output))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d765d75d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "NUM_OF_DISTS = 2\n",
    "KL_DIV_VALUES = [0.25, 0.5, -0.25, -0.5]\n",
    "\n",
    "def get_completions_for_kl_val(model: Model, prompts, f: Feature, kl, neg=False, sae=None):\n",
    "    act = get_activation_for_kl(model, prompts, f, kl, neg=neg, sae=sae)\n",
    "    positive = hooked_gen(model.m.to_tokens(prompts), model, f, n=25, value=act, temperature=0.75, sae=sae)\n",
    "    return [x.replace(\"\\n\", \"\\\\n\").replace(\"\\r\", \"\\\\r\") for x in positive]\n",
    "\n",
    "def get_random_amps(model: Model):\n",
    "    with open(f\"output_metric/{model.model_id}_random_amps.pkl\", \"rb\") as f:\n",
    "        random_amps = pickle.load(f)\n",
    "        \n",
    "    return random.sample(random_amps, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3077ee3a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_random_amps(model: Model, num, sae):\n",
    "    random_amps = [\"\" for _ in range(NUM_OF_DISTS)]\n",
    "    for i in KL_DIV_VALUES:\n",
    "        for x in range(num):\n",
    "            ampy = random_amps[x]\n",
    "            random_feature = random.sample(range(sae.cfg.d_sae), 1)[0]\n",
    "            # TODO: f = Feature(model.model_id, random_feature, layer...)\n",
    "            rand_pos = get_completions_for_kl_val(model, OUTPUT_METRIC_GENERATION_PROMPTS, random_feature, i, sae=sae)\n",
    "            ampy += f\"\\n<{'+' if i >= 0 else ''}{i}>\" + f\"\\n<{'+' if i >= 0 else ''}{i}>\".join([f\"'{OUTPUT_METRIC_GENERATION_PROMPTS[x]}': '{rand_pos[x]}'\" for x in range(len(rand_pos))])\n",
    "            random_amps[x] = ampy\n",
    "\n",
    "    return random_amps\n",
    "\n",
    "def write_random_amps(model: Model, num):\n",
    "    all_random_amps = []\n",
    "    for _ in range(10):\n",
    "        # TODO: get random SAE for model\n",
    "        sae = None\n",
    "        random_amps = generate_random_amps(model, num, sae)\n",
    "        all_random_amps.extend(random_amps)\n",
    "            \n",
    "    with open(f\"output_metric/{model.model_id}_random_amps.pkl\", \"wb\") as f:\n",
    "        pickle.dump(random_amps, f)\n",
    "\n",
    "    return random_amps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278742ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class OutputScore:\n",
    "    correct_choice: int\n",
    "    chosen_index: int\n",
    "\n",
    "    def success(self) -> bool:\n",
    "        # the output metric final test for success\n",
    "        return self.correct_choice == self.chosen_index\n",
    "    \n",
    "def to_int(s):\n",
    "    try:\n",
    "        return int(re.search(r'\\d+', s).group())\n",
    "    except: \n",
    "        print(f\"No int in {s}\")\n",
    "        raise\n",
    "\n",
    "def get_output_score(description: str, m: Model, f: Feature, sae=None) -> OutputScore:\n",
    "    amps = \"\"\n",
    "    for i in KL_DIV_VALUES:\n",
    "        pos = get_completions_for_kl_val(m, OUTPUT_METRIC_GENERATION_PROMPTS, f, abs(i), neg=False if i>=0 else True, sae=sae)\n",
    "        amps += f\"\\n<{'+' if i >= 0 else ''}{i}>\" + f\"\\n<{'+' if i >= 0 else ''}{i}>\".join([f\"'{OUTPUT_METRIC_GENERATION_PROMPTS[x]}': '{pos[x]}'\" for x in range(len(pos))])\n",
    "\n",
    "    random_amps = get_random_amps(m)\n",
    "    random_amps = random_amps.copy()\n",
    "    random_amps.append(amps)\n",
    "\n",
    "    random_indices = random.sample(range(NUM_OF_DISTS+1), NUM_OF_DISTS+1)\n",
    "    ordered_amps = [random_amps[i] for i in random_indices]\n",
    "    correct_choice = random_indices.index(NUM_OF_DISTS) + 1\n",
    "    \n",
    "    prompt = STEERING_FULL_PROMPT_TEMPLATE.format(explanation=description, amplifications1=ordered_amps[0], amplifications2=ordered_amps[1], amplifications3=ordered_amps[2])\n",
    "    response = explainer([\n",
    "        {\"role\": \"user\", \"content\": OUTPUT_METRIC_SYS_PROMPT + \"\\n\\n\" + prompt}\n",
    "    ], weak=True)\n",
    "\n",
    "    chosen_index = to_int(response.splitlines()[-1])\n",
    "    return OutputScore(correct_choice, chosen_index)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6763dd",
   "metadata": {},
   "source": [
    "## Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae13304c",
   "metadata": {},
   "source": [
    "### Getting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626899c6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_all_features_sample(m: Model, n: int = None, in_file: str = None, start_chunk: int = 0) -> List[Feature]:\n",
    "    all_features_sample = m.get_features(in_file=in_file)\n",
    "    if not m.with_sae:\n",
    "        logger.info(f\"Got {len(all_features_sample)} features for {m.model_id}\")\n",
    "        return all_features_sample\n",
    "    \n",
    "    features_count = len(all_features_sample)\n",
    "    no_act_count = len([x for x in all_features_sample if x.activations == []])\n",
    "    \n",
    "    logger.info(f\"Loaded {features_count} features, out of which {no_act_count} have no activations data\")\n",
    "    \n",
    "    # validate n according to feature amount and chunks\n",
    "    if n is None:\n",
    "        n = features_count\n",
    "    types_count = len(m.sae_types) \n",
    "    sizes_count = len(m.sae_sizes)\n",
    "    assert n <= features_count, f\"n must be less or equal to the numbers of sample features {features_count}\"\n",
    "    logger.info(f\"Getting {n}/{features_count} features with {types_count} types and {sizes_count} sizes\")\n",
    "    \n",
    "    chunks_count = types_count * sizes_count\n",
    "    chunk_size = n // chunks_count\n",
    "    assert n % chunks_count == 0, f\"n must be divisible by {chunks_count} so that groups can be equal, for example {n - (n % chunks_count)}\"\n",
    "    logger.info(f\"Dividing into {chunks_count} chunks, each of size {chunk_size}\")\n",
    "\n",
    "    # generate equally distribued sample made of chunks\n",
    "    sample = []\n",
    "    for t in m.sae_types:\n",
    "        for s in m.sae_sizes:\n",
    "            logger.debug(f\"Creating chunk for {t}/{s} with range [{start_chunk}:{start_chunk + chunk_size}]\")\n",
    "            chunk = [x for x in all_features_sample if x.type == t and x.size == s][start_chunk:start_chunk + chunk_size]\n",
    "            assert len(chunk) == chunk_size, f\"current chunk length ({len(chunk)}) is not enough ({chunk_size})\"\n",
    "            sample.extend(chunk[:chunk_size])\n",
    "    random.shuffle(sample)\n",
    "    logger.info(f\"Got {len(sample)} final sample features for {m.model_id}\")\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26759bc",
   "metadata": {},
   "source": [
    "### Main analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0b4e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Result:\n",
    "    layer: str\n",
    "    feature: str\n",
    "    sae_type: str\n",
    "    sae_size: str\n",
    "    dashboard_url: str\n",
    "\n",
    "    # MA\n",
    "    max_act_exp: str\n",
    "    max_act_input_success: bool\n",
    "    max_act_output_success: bool\n",
    "    \n",
    "    # VP\n",
    "    vocab_proj_exp: str\n",
    "    vocab_proj_input_success: bool\n",
    "    vocab_proj_output_success: bool\n",
    "\n",
    "    # TC\n",
    "    token_change_exp: str\n",
    "    token_change_input_success: bool\n",
    "    token_change_output_success: bool\n",
    "\n",
    "    # RAW (VP + MA)\n",
    "    ensemble_raw_vm_exp: str\n",
    "    ensemble_raw_vm_input_success: bool\n",
    "    ensemble_raw_vm_output_success: bool\n",
    "\n",
    "    # RAW (TC + MA)\n",
    "    ensemble_raw_tm_exp: str\n",
    "    ensemble_raw_tm_input_success: bool\n",
    "    ensemble_raw_tm_output_success: bool\n",
    "\n",
    "    # RAW (VP + TC)\n",
    "    ensemble_raw_vt_exp: str\n",
    "    ensemble_raw_vt_input_success: bool\n",
    "    ensemble_raw_vt_output_success: bool\n",
    "    \n",
    "    # RAW ALL (VP + TC + MA)\n",
    "    ensemble_raw_all_exp: str\n",
    "    ensemble_raw_all_input_success: bool\n",
    "    ensemble_raw_all_output_success: bool\n",
    "\n",
    "    # CONCAT ALL (VP + TC + MA)\n",
    "    ensemble_concat_all_exp: str\n",
    "    ensemble_concat_all_input_success: bool\n",
    "    ensemble_concat_all_output_success: bool\n",
    "\n",
    "    @classmethod\n",
    "    def from_row(cls, row):\n",
    "        return cls(**vars(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f979248",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_feature(f: Feature, m: Model) -> Result:\n",
    "    logger.trace(f\"Analyzing {f} with {f.sae_id}\") if m.with_sae else logger.trace(f\"Analyzing {f}\")\n",
    "    \n",
    "    if m.with_sae:\n",
    "        sae, _, _ = SAE.from_pretrained(release = f.sae_release, sae_id = f.sae_id, device = device)\n",
    "        sae_model_id = \"meta-llama/Llama-3.1-8B\" if m.is_llama() else m.model_id\n",
    "        assert sae.cfg.model_name == sae_model_id, f\"sae model name {sae.cfg.model_name} doesn't match current model {sae_model_id}!\"\n",
    "        if f.size != \"\":\n",
    "            assert sae.cfg.d_sae == f.get_size_int(), f\"sae size {sae.cfg.d_sae} doesn't match current feature {f.get_size_int()}!\"\n",
    "        \n",
    "        try:\n",
    "            np_sae_id = m.get_np_sae_id(f.layer, f.type, f.size)\n",
    "            dashboard_url = f.get_pedia_dashboard_url(np_sae_id)\n",
    "        except:\n",
    "            logger.warning(f\"get_np_sae_id() failed\")\n",
    "            dashboard_url = \"\"\n",
    "    else:\n",
    "        sae = None\n",
    "        dashboard_url = \"\"\n",
    "        \n",
    "        \n",
    "    ### DESCRIPTION GENERATION ###\n",
    "    \n",
    "    # 1 - MaxAct (MA)\n",
    "    if not f.activations:\n",
    "        max_act_user_prompt = \"No activations found for feature.\"\n",
    "        max_act_exp = \"NO ACTIVATIONS DATA\"\n",
    "    else:\n",
    "        max_act_user_prompt = generate_max_act_user_prompt(f, activating_examples=f.activations)\n",
    "        max_act_exp = get_description(MAX_ACT_SYS_PROMPT, max_act_user_prompt)\n",
    "        \n",
    "    # 2 - VocabProj (VP)\n",
    "    top_tokens, bottom_tokens = get_dec_unembed(m, f, sae=sae)\n",
    "    vocab_proj_user_prompt = VOACB_PROJ_USER_PROMPT.format(top_tokens + bottom_tokens)\n",
    "    vocab_proj_exp = get_description(VOCAB_PROJ_SYS_PROMPT, vocab_proj_user_prompt)\n",
    "    \n",
    "    # 3 - TokenChange (TC)\n",
    "    real, random1, random2, random3, random4 = get_causal_data(m, f, sae=sae)\n",
    "    token_change_user_prompt = generate_token_change_user_prompt(real)\n",
    "    token_change_exp = get_description(TOKEN_CHANGE_SYS_PROMPT, token_change_user_prompt)\n",
    "    \n",
    "    # 4 - Ensemble Raw (VP + MA = VM)\n",
    "    ensemble_raw_vm_exp = get_description(ENSEMBLE_RAW_VM_SYS_PROMPT, ENSEMBLE_IO_USER_PROMPT.format(\n",
    "        input_data=max_act_user_prompt, output_data=top_tokens + bottom_tokens))\n",
    "    \n",
    "    # 5 - Ensemble Raw (TC + MA = TM)\n",
    "    ensemble_raw_tm_exp = get_description(ENSEMBLE_RAW_TM_SYS_PROMPT, ENSEMBLE_IO_USER_PROMPT.format(\n",
    "        input_data=max_act_user_prompt, output_data=token_change_user_prompt))\n",
    "    \n",
    "    # 6 - Ensemble Raw (VP + TC = VT)\n",
    "    vocab_proj_user_prompt = VOACB_PROJ_USER_PROMPT.format(top_tokens + bottom_tokens + list(set(real)))\n",
    "    ensemble_raw_vt_exp = get_description(VOCAB_PROJ_SYS_PROMPT, vocab_proj_user_prompt)\n",
    "    \n",
    "    # 7 - Ensemble Raw (All)\n",
    "    ensemble_raw_all_exp = get_description(ENSEMBLE_RAW_ALL_SYS_PROMPT, ENSEMBLE_IOO_USER_PROMPT.format(\n",
    "        input_data=max_act_user_prompt, output_data1=top_tokens + bottom_tokens, output_data2=token_change_user_prompt))\n",
    "    \n",
    "    # 8 - Ensemble Concat (All)\n",
    "    ensemble_concat_all_exp = f\"{vocab_proj_exp}\\n{max_act_exp}\\n{token_change_exp}\"\n",
    "    logger.trace(\"Got descriptions\")\n",
    "\n",
    "    ### DESCRIPTION EVALUATION ###\n",
    "\n",
    "    ma_in_score = get_input_score(max_act_exp, m, f, sae=sae)\n",
    "    vp_in_score = get_input_score(vocab_proj_exp, m, f, sae=sae)\n",
    "    tc_in_score = get_input_score(token_change_exp, m, f, sae=sae)\n",
    "    ens_raw_vm_in_score = get_input_score(ensemble_raw_vm_exp, m, f, sae=sae)\n",
    "    ens_raw_tm_in_score = get_input_score(ensemble_raw_tm_exp, m, f, sae=sae)\n",
    "    ens_raw_vt_in_score = get_input_score(ensemble_raw_vt_exp, m, f, sae=sae)\n",
    "    ens_raw_all_in_score = get_input_score(ensemble_raw_all_exp, m, f, sae=sae)\n",
    "    ens_con_all_in_score = get_input_score(ensemble_concat_all_exp, m, f, sae=sae)\n",
    "    logger.trace(\"Got input score\")\n",
    "    \n",
    "    ma_out_score = get_output_score(max_act_exp, m, f, sae=sae)\n",
    "    vp_out_score = get_output_score(vocab_proj_exp, m, f, sae=sae)\n",
    "    tc_out_score = get_output_score(token_change_exp, m, f, sae=sae)\n",
    "    ens_raw_vm_out_score = get_output_score(ensemble_raw_vm_exp, m, f, sae=sae)\n",
    "    ens_raw_tm_out_score = get_output_score(ensemble_raw_tm_exp, m, f, sae=sae)\n",
    "    ens_raw_vt_out_score = get_output_score(ensemble_raw_vt_exp, m, f, sae=sae)\n",
    "    ens_raw_all_out_score = get_output_score(ensemble_raw_all_exp, m, f, sae=sae)\n",
    "    ens_con_all_out_score = get_output_score(ensemble_concat_all_exp, m, f, sae=sae)\n",
    "    logger.trace(\"Got output score\")\n",
    "    \n",
    "    layer = sae.cfg.hook_layer if sae is not None else f.layer\n",
    "    result = Result(\n",
    "        layer=layer, feature=f.feature, sae_type=f.type, sae_size=f.size, dashboard_url=dashboard_url,\n",
    "        \n",
    "        max_act_exp=max_act_exp, max_act_input_success=ma_in_score.success(), max_act_output_success=ma_out_score.success(),\n",
    "        vocab_proj_exp=vocab_proj_exp, vocab_proj_input_success=vp_in_score.success(), vocab_proj_output_success=vp_out_score.success(),\n",
    "        token_change_exp=token_change_exp, token_change_input_success=tc_in_score.success(), token_change_output_success=tc_out_score.success(),\n",
    "        ensemble_raw_vm_exp=ensemble_raw_vm_exp, ensemble_raw_vm_input_success=ens_raw_vm_in_score.success(), ensemble_raw_vm_output_success=ens_raw_vm_out_score.success(),\n",
    "        ensemble_raw_tm_exp=ensemble_raw_tm_exp, ensemble_raw_tm_input_success=ens_raw_tm_in_score.success(), ensemble_raw_tm_output_success=ens_raw_tm_out_score.success(),\n",
    "        ensemble_raw_vt_exp=ensemble_raw_vt_exp, ensemble_raw_vt_input_success=ens_raw_vt_in_score.success(), ensemble_raw_vt_output_success=ens_raw_vt_out_score.success(),\n",
    "        ensemble_raw_all_exp=ensemble_raw_all_exp, ensemble_raw_all_input_success=ens_raw_all_in_score.success(), ensemble_raw_all_output_success=ens_raw_all_out_score.success(),\n",
    "        ensemble_concat_all_exp=ensemble_concat_all_exp, ensemble_concat_all_input_success=ens_con_all_in_score.success(), ensemble_concat_all_output_success=ens_con_all_out_score.success()\n",
    "        )\n",
    "    \n",
    "    # free memory\n",
    "    del sae\n",
    "    free()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ee635a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_df_scores(df):\n",
    "    ma_input = len(df[df[\"Max Act Input Success\"] == True]) / len(df)\n",
    "    vp_input = len(df[df[\"Vocab Proj Input Success\"] == True]) / len(df)\n",
    "    tc_input = len(df[df[\"Token Change Input Success\"] == True]) / len(df)\n",
    "    ens_raw_vm_input = len(df[df[\"Ensemble Raw Vm Input Success\"] == True]) / len(df)\n",
    "    ens_raw_tm_input = len(df[df[\"Ensemble Raw Tm Input Success\"] == True]) / len(df)\n",
    "    ens_raw_vt_input = len(df[df[\"Ensemble Raw Vt Input Success\"] == True]) / len(df)\n",
    "    ens_raw_all_input = len(df[df[\"Ensemble Raw All Input Success\"] == True]) / len(df)\n",
    "    ens_con_all_input = len(df[df[\"Ensemble Concat All Input Success\"] == True]) / len(df)\n",
    "\n",
    "    input_results = { \n",
    "        \"Max Act Input Result\": ma_input,\n",
    "        \"Vocab Proj Input Result\": vp_input,\n",
    "        \"Token Change Input Result\": tc_input,\n",
    "        \"Ensemble Raw Vm Input Result\": ens_raw_vm_input,\n",
    "        \"Ensemble Raw Tm Input Result\": ens_raw_tm_input,\n",
    "        \"Ensemble Raw Vt Input Result\": ens_raw_vt_input,\n",
    "        \"Ensemble Raw All Input Result\": ens_raw_all_input,\n",
    "        \"Ensemble Concat All Input Result\": ens_con_all_input\n",
    "    }\n",
    "    \n",
    "    return input_results\n",
    "\n",
    "def get_knowledge_exp_df(m: Model, features: List[Feature], limit=None, start=0, output_csv=None):\n",
    "    exp_results = DefaultDict(list)\n",
    "    features = features[start:]\n",
    "    if limit:\n",
    "        features = features[:limit]\n",
    "\n",
    "    pbar = Pbar(total = len(features))\n",
    "    for i, f in enumerate(features):\n",
    "        try:\n",
    "            # full analysis for the current feature (explainer + scorer)\n",
    "            result = analyze_feature(f, m)\n",
    "            if not result:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            # append all results to dataframe, figure out the fields dynamically\n",
    "            for k, v in asdict(result).items():\n",
    "                df_column = k.replace(\"_\", \" \").title()\n",
    "                exp_results[df_column].append(v)\n",
    "        \n",
    "            if i % 5 == 0:\n",
    "                df = pd.DataFrame(exp_results)\n",
    "                results = get_df_scores(df)\n",
    "                pbar.set_description(results)\n",
    "                if output_csv:\n",
    "                    df.to_csv(output_csv, index=False)\n",
    "            pbar.update(1)\n",
    "\n",
    "        except (JSONDecodeError, KeyError, AttributeError, ValueError, IndexError) as e:\n",
    "            logger.error(f\"Error with {f.sae_id}/{f.feature}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            free()\n",
    "            continue\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(e)\n",
    "            traceback.print_exc()\n",
    "            break\n",
    "\n",
    "    df = pd.DataFrame(exp_results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30f3dba",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(EXPERIMENTS_DIR):\n",
    "    os.mkdir(EXPERIMENTS_DIR)\n",
    "\n",
    "def analyze_model(m: Model, fs: List[Feature], limit=None, csv_path=\"\") -> str:\n",
    "    current_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    fs_len = limit if limit else len(fs)\n",
    "    file_name = f'{m.model_name}_{fs_len}_features_@{current_time}'\n",
    "    \n",
    "    log_full_path = os.path.join(LOGS_DIR, f\"{file_name}.log\")\n",
    "    log_sink = logger.add(log_full_path, level=\"TRACE\")\n",
    "    \n",
    "    try:\n",
    "        csv_full_path = os.path.join(EXPERIMENTS_DIR, f\"{file_name}.csv\") if not csv_path else csv_path\n",
    "        df = get_knowledge_exp_df(m, fs, limit=limit, output_csv=csv_full_path)\n",
    "        df.to_csv(csv_full_path)\n",
    "        logger.info(f\"Results saved to {csv_full_path}\")\n",
    "        return csv_full_path\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "    finally:\n",
    "        logger.remove(log_sink)\n",
    "    \n",
    "def calculate_score(experiment_csv):\n",
    "    df = pd.read_csv(experiment_csv)\n",
    "    results = get_df_scores(df)\n",
    "    logger.info(results)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5146a207",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8320dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOSEN_MODEL = \"gemma\"\n",
    "\n",
    "def get_model_obj(chosen_model: str) -> Model:\n",
    "    models_map = { \n",
    "        \"gemma\": MODELS[\"GEMMA-2-2B\"],\n",
    "        \"llama\": MODELS[\"LLAMA-3.1-8B\"], \n",
    "        \"gpt\": MODELS[\"GPT-2-SM-V5\"],\n",
    "        \"llama_in\": MODELS[\"LLAMA-3.1-8B-IN\"]}\n",
    "    return models_map[chosen_model]\n",
    "\n",
    "model = get_model_obj(CHOSEN_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea39a320",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba5cd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model.with_sae:\n",
    "    specific_layers = model.get_specific_layers()\n",
    "    model.get_saes_info_specific_layers(specific_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed09c602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.write_feature_sample()\n",
    "all_features_sample = get_all_features_sample(model)\n",
    "all_features_sample[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182e66ba",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "results_csv = analyze_model(model, all_features_sample, limit=1)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
